<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"/>



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2"/>























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/header-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/header-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.7.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Encoder-Decoder大多数注意力模型都是在Encoder-Decoder框架下发挥作用，Encoder-Decoder模型一般可以用来做机器翻译、文本摘要生成、问答系统、聊天机器人等。">
<meta name="keywords" content="deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention">
<meta property="og:url" content="https:&#x2F;&#x2F;tianhongzxy.github.io&#x2F;2019&#x2F;11&#x2F;19&#x2F;DL%E4%B8%AD%E7%9A%84Attention&#x2F;index.html">
<meta property="og:site_name" content="TianHongZXY">
<meta property="og:description" content="Encoder-Decoder大多数注意力模型都是在Encoder-Decoder框架下发挥作用，Encoder-Decoder模型一般可以用来做机器翻译、文本摘要生成、问答系统、聊天机器人等。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11Fph6.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FFje.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FStx.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FP1O.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FC9K.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FicD.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11FjxS.md.jpg">
<meta property="og:updated_time" content="2020-01-30T10:53:47.254Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;s2.ax1x.com&#x2F;2020&#x2F;01&#x2F;30&#x2F;11Fph6.jpg">






  <link rel="canonical" href="https://TianHongZXY.github.io/2019/11/19/DL中的Attention/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Attention | TianHongZXY</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">TianHongZXY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">那时我们一无所有，也没有什么能妨碍我们享受静夜</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://TianHongZXY.github.io/2019/11/19/DL%E4%B8%AD%E7%9A%84Attention/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TianHongZXY"/>
      <meta itemprop="description" content="浪漫骑士 行吟诗人 自由思想者"/>
      <meta itemprop="image" content="/images/header.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianHongZXY"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Attention

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-11-19 22:14:51" itemprop="dateCreated datePublished" datetime="2019-11-19T22:14:51+08:00">2019-11-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2020-01-30 18:53:47" itemprop="dateModified" datetime="2020-01-30T18:53:47+08:00">2020-01-30</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/11/19/DL%E4%B8%AD%E7%9A%84Attention/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">Comments: </span> <span class="post-comments-count valine-comment-count" data-xid="/2019/11/19/DL%E4%B8%AD%E7%9A%84Attention/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>大多数注意力模型都是在Encoder-Decoder框架下发挥作用，Encoder-Decoder模型一般可以用来做机器翻译、文本摘要生成、问答系统、聊天机器人等。<a id="more"></a>Encoder的输入是Source，Decoder要输出的是Target，一般通过RNN来编码输入的句子，得到一个包含的句子语义信息的Vector $C$。假设句子由$t$个单词组成，即$(w_{1},w_{2},w_{3},···,w_{t-t-1},w_{t})$。那么RNN或Transformer等模型就作为编码句子的Encoder，首先句子经过Embedding层，从one-hot形式变为词向量$(x_{1},x_{2},x_{3},···,x_{t-1},x_{t})$，假设它提取句子的语义信息的函数为$\psi$，则：</p>
<script type="math/tex; mode=display">
C = \psi(x_{1},x_{2},x_{3},···,x_{t-1},x_{t})</script><p>对于Decoder来说，它的目标是根据Encoder编码好的句子信息$C$解码出我们想要的结果，这结果可以是对原输入句子的回答，翻译，或摘要等。假设Decoder的输出为$(y_{1},y_{2},y_{3},···,y_{m-1},y_{m})$，$m$和$t$的大小关系并不确定，一般来说Decoder输出<code>&lt;EOS&gt;</code>句子才算结束。</p>
<p>假设t=i时，我们要输出$y_{i}$，其实我们输出的是一个概率分布，然后再选取概率最大的那个单词当作$y_{i}$输出(<strong>贪心 search</strong>)，还有另一种方式叫<strong>beam search</strong>，这个不是本文重点就在此不多说了。根据<em>Bahdanau et al. ICLR2014</em>第一次提出的将Attention运用于机器翻译任务的论文中，$y_{i}$的计算公式如下：</p>
<script type="math/tex; mode=display">
p(y_{i}|y_{1},···,y_{i-1},x) = g(y_{i-1},s_{i},C)</script><p>其中$s_{i}$是RNN在$i^{th}$timestep的hidden state，由如下公式计算：</p>
<script type="math/tex; mode=display">
s_{i} = f(s_{i-1},y_{i-1},C)</script><p>由于直接输出的$y_{i}$实际是一个长度为vocabulary size $|V|$的vector，因此最终根据生成的单词$y_{i}$其实应该变成一个one-hot vector：</p>
<script type="math/tex; mode=display">
y_{i} = \mathop{onehot}(y_{i})，y_{i} \in R^{|V|}</script><p>在这里简单地将Decoder整个生成yi的函数用$\varphi$表示：</p>
<script type="math/tex; mode=display">
y_{i} = \varphi(C,y_{1},y_{2},y_{3},···,y_{i-1})</script><p>这样就是一个Encoder-Decoder框架运作的基本方式，更直观的可以参见下图。</p>
<p><img src="https://s2.ax1x.com/2020/01/30/11Fph6.jpg" alt="11Fph6.jpg" border="0" /></p>
<h1 id="Soft-Attention"><a href="#Soft-Attention" class="headerlink" title="Soft Attention"></a>Soft Attention</h1><p>最常见也应用最广泛的Attention就是Soft Attention，上面的Encoder-Decoder框架，在Decoder生成每一个yi时，对原输入整个句子语义信息C都给予了同等的注意力，即原句中不同的单词对于生成每一个yi的贡献是相同的。这明显是有问题的，比如在中英翻译：“我今天吃了一个苹果”，“I ate an apple today”，在翻译apple这个词时，原句中的“苹果”对其生成apple要比其他词都重要，因此，需要一个给单纯的Encoder-Decoder模型融入更多的知识，那就是Attention。</p>
<p>Attention的有效性和作用是很intuitive的，比如人在读文章、观察物体时也是会有注意力的参与的，不可能读一页书读到第一行，还能同时注意第二十行的句子，注意力肯定是分配在某个局部的句子上的。因此，给Encoder-Decoder添加Attention，就是要让Decoder在生成$y_{i}$时，给输入句子的单词$(x_{1},x_{2},x_{3},···,x_{t-1},x_{t})$分配不同的注意力权重$\alpha_{ij}$，权重代表着单词$x_{j}$对生成$y_{i}$的重要性。</p>
<p>假设Encoder是RNN，输入每个单词$x_{j}$后都会输出一个隐状态$h_{j}$，那么对生成$y_{i}$时，原先对生成每个yi都是相同的句子语义表示$C$会被替换成根据当前要生成的单词yi而不断变化的$C_{i}$。理解Attention模型的关键就是这里，即把固定的句子语义表示$C$变成了根据当前要输出的单词yi来进行调整的、融入注意力知识的、不断变化的$C_{i}$。</p>
<p>因此上面生成yi的式子变化成如下形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
s_{i} &= f(s_{i-1},y_{i-1},C_{i}) \\
e_{ij} &= \Gamma(s_{i-1}, h_{j}) \\
\alpha_{ij} &= \frac{exp(e_{ij})}{\sum_{k=1}^{t}exp(e_{ik})}\\
C_{i} &= \sum_{j=1}^{t}\alpha_{ij}\cdot h_{j} \\
p(y_{i}) &= g(y_{i-1},s_{i},C_{i}) \\

\end{aligned}</script><p>其中，t代表输入句子Source的长度，$\alpha_{ij}$代表在Decoder输出第i个单词时给Source中第j个单词的注意力分配系数，而hj则是Encoder输入第j个单词时输出的隐状态(语义编码)。根据NMT论文原文，$\Gamma$ <strong>is an alignment model which scores how well the inputs around position j and the output at position i match. The score is based on the RNN hidden state $s_{i−1}$ (just before emitting yi) and the j-th annotation hj of the input sentence. </strong>至于$\Gamma$函数的选取下面会说明。下图是一个可视化的效果，帮助理解。</p>
<p><img src="https://s2.ax1x.com/2020/01/30/11FFje.jpg" alt="11FFje.jpg" border="0" /></p>
<h1 id="计算Attention"><a href="#计算Attention" class="headerlink" title="计算Attention"></a>计算Attention</h1><p>我们已经知道了attention是什么，有什么作用，下面具体说明到底怎么计算attention。</p>
<p>假设Encoder输入$x_{j}$后输出的隐状态为$h_{j}$，Decoder在输出$y_{i}$前隐层节点状态为$s_{i-1}$，那么可以用这个时刻的隐层节点状态去一一和输入句子中每个单词对应的RNN隐状态$h_{j}$进行对比，即通过函数$\Gamma(h_{j},s_{i-1})$来获得目标单词yi和每个输入单词对齐的可能性。$\Gamma$函数在不同论文里可能会采取不同的方法，然后$\Gamma$的所有输出经过Softmax进行归一化就得到了注意力分配到每个输入单词上的权重。下图展示了这个计算过程：</p>
<p><img src="https://s2.ax1x.com/2020/01/30/11FStx.jpg" alt="11FStx.jpg" border="0" /></p>
<p>上面说到不同的论文$\Gamma$函数会采取不同的方法，其中较为普遍的类型有两种，一个是加法Attention，另一个是乘法Attention。</p>
<h2 id="加法Attention"><a href="#加法Attention" class="headerlink" title="加法Attention"></a>加法Attention</h2><script type="math/tex; mode=display">
\Gamma\left(\mathbf{h}_{j}, \mathbf{s}_{i-1}\right)=\mathbf{v}_{a}^{\top} \tanh \left(\mathbf{W}_{a}\left[\mathbf{h}_{j} ; \mathbf{s}_{i-1}\right]\right)</script><p>$v_{a}$和$W_{a}$为可训练的参数，$v_a$将计算结果变成一个scalar，h与s之间的分号表示将二者concatenate到一起，产生一个更长的vector，然后和$W_a$做矩阵乘法。最后再把得到的value一起送往softmax层，进而产生一个符合概率分布的attention。</p>
<h2 id="乘法Attention"><a href="#乘法Attention" class="headerlink" title="乘法Attention"></a>乘法Attention</h2><script type="math/tex; mode=display">
\Gamma\left(h_{j}, s_{i-1}\right)=h_{j}^{\top} \mathbf{W}_{a} s_{i-1}</script><p>将加法和乘法排列组合变换，就能得到另一种方式——多重感知机(multi-layer perceptron)</p>
<script type="math/tex; mode=display">
\Gamma\left(h_{j}, s_{i-1}\right)= MLP(h_{j},s_{i-1})</script><p>在代码实现中，运用矩阵运算，可以大大加速计算，这里的$h_{j}$不再是单个vector，而是$h_{1}\sim h_{t}$组成的一个矩阵$M$，与$s_{i-1}$一同计算后，得到了一个长度为$t$的vector $\alpha_{i}$，它代表着在生成$y_{i}$时，对$h_{1}\sim h_{t}$分配的注意力权重。</p>
<p>下图展示了在文本生成式摘要时，注意力的分配</p>
<p><img src="https://s2.ax1x.com/2020/01/30/11FP1O.jpg" alt="11FP1O.jpg" border="0" /></p>
<h1 id="Attention机制的本质思想"><a href="#Attention机制的本质思想" class="headerlink" title="Attention机制的本质思想"></a>Attention机制的本质思想</h1><p><img src="https://s2.ax1x.com/2020/01/30/11FC9K.jpg" alt="11FC9K.jpg" border="0" /></p>
<p>我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，可以得到每个Key针对该Query，Value的分配到的权重系数，然后对所有Key的Value进行加权求和，便得到了最终的Attention值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p>
<script type="math/tex; mode=display">
Attention(Query, Source) = \sum_{i}^{L_{x}}Similarity(Query, Key_{i}) \cdot Value_{i}</script><p>上式，$L_{x}$代表Source的长度。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value其实是同一个东西，即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<p>也可以将Attention机制看作一种软寻址(Soft Addressing)：Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention值。通过Query和存储器内元素Key的地址进行相似性比较来寻址。之所以说是软寻址，是因为不像一般寻址只从存储内容里面找出一条内容，而是从每个Key地址都可能会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以得到最终的Value值，也即Attention值。</p>
<p>Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为下图展示的三个阶段。</p>
<p><img src="https://s2.ax1x.com/2020/01/30/11FicD.jpg" alt="11FicD.jpg" border="0" /></p>
<p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Keyi，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，计算公式分别如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Similarity(Query, Key_{i}) &= Query^{\top}\cdot Key_{i} \\
Similarity(Query, Key_{i}) &= \frac{Query^{\top}\cdot Key_{i}}{||Query||\cdot ||Key_{i}||} \\
Similarity(Query, Key_{i}) &= MLP(Query，Key_{i})
\end{aligned}</script><p>接着将得到的数值进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。一般采用如下公式计算：</p>
<script type="math/tex; mode=display">
\alpha_{i} = Softmax(Sim_{i}) = \frac{e^{Sim_{i}}}{\sum_{j}^{L_{x}}e^{Sim_{j}}}</script><p>最后得到Query关于Source的加权后的Value值：</p>
<script type="math/tex; mode=display">
Value(Query，Source) = \sum_{i}^{L_{x}}\alpha_{i}\cdot Value_{i}</script><h1 id="延伸阅读"><a href="#延伸阅读" class="headerlink" title="延伸阅读"></a>延伸阅读</h1><p>Attention的变体有非常非常多，针对不同任务，不同的Attention机制也有不同的效果，下面最近较火的三种Attention，相信读完上面的内容，理解下面的Attention不再是难题。</p>
<ul>
<li><p>Self-Attention</p>
</li>
<li><p>Hierarchical-Attention</p>
</li>
<li><p>Co-Attention</p>
</li>
</ul>
<p>参考链接：</p>
<ol>
<li><p><a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37601161</a></p>
</li>
<li><p><a href="https://www.jianshu.com/p/c94909b835d6" target="_blank" rel="noopener">https://www.jianshu.com/p/c94909b835d6</a></p>
</li>
</ol>
<p><a href="https://imgchr.com/i/11FjxS" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2020/01/30/11FjxS.md.jpg" width="200" height="230" align="center" alt="11FjxS.jpg" border="0"  /></a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/11/17/NLP%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/" rel="next" title="NLP中的对抗训练">
                <i class="fa fa-chevron-left"></i> NLP中的对抗训练
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/30/RNN&LSTM&GRU/" rel="prev" title="循环神经网络">
                循环神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="TianHongZXY"/>
            
              <p class="site-author-name" itemprop="name">TianHongZXY</p>
              <p class="site-description motion-element" itemprop="description">浪漫骑士 行吟诗人 自由思想者</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/%20%7C%7C%20archive">
                
                    <span class="site-state-item-count">31</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/TianHongZXY" title="GitHub &rarr; https://github.com/TianHongZXY" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/mailto:tianhongzxy@163.com" title="E-Mail &rarr; mailto:tianhongzxy@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.zhihu.com/people/tianhongzxy" title="知乎 &rarr; https://www.zhihu.com/people/tianhongzxy" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-number">1.</span> <span class="nav-text">Encoder-Decoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-Attention"><span class="nav-number">2.</span> <span class="nav-text">Soft Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算Attention"><span class="nav-number">3.</span> <span class="nav-text">计算Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#加法Attention"><span class="nav-number">3.1.</span> <span class="nav-text">加法Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#乘法Attention"><span class="nav-number">3.2.</span> <span class="nav-text">乘法Attention</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention机制的本质思想"><span class="nav-number">4.</span> <span class="nav-text">Attention机制的本质思想</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#延伸阅读"><span class="nav-number">5.</span> <span class="nav-text">延伸阅读</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span>
  <!-- <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span> -->
  <span class="author" itemprop="copyrightHolder"><span class="with-love"><i class="fa fa-heart-o"></i></span>TianHongZXY</span>

  

  
</div>
<!--

  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v6.7.0</div>

-->


        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  

  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  








  
  
  
  
  <script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  <script>
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item) > -1;
    });
    new Valine({
      el: '#comments' ,
      verify: false,
      notify: false,
      appId: 'fNjvG519rNMQCRTez4XP1aGe-gzGzoHsz',
      appKey: 'DY0fa7oCKanbyW5J4UoxG9ug',
      placeholder: 'Say something',
      avatar: 'mm',
      meta:guest,
      pageSize: '10' || 10,
      visitor: false
    });
  </script>




  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>
  <script>
    
    
  </script>


  

  

  

  

  

  

  

  
  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/click.js"></script>
</body>
</html>
