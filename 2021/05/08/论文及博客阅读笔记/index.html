<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"/>



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2"/>























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/header-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/header-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.7.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="小样本学习 CLIP  一般的图像分类是把图片用backbone编码成一个feature vector，然后用一个classifier weight matrix W 乘上得到分类结果，$W_i$是class i的 prototype weight vector   f=\operatorname{Backbone}(\mathbf{I}), \quad p_{i}=\frac{\exp \lef">
<meta name="keywords" content="deep learning,machine learning - NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="论文及博客阅读笔记（持续更新）">
<meta property="og:url" content="https:&#x2F;&#x2F;tianhongzxy.github.io&#x2F;2021&#x2F;05&#x2F;08&#x2F;%E8%AE%BA%E6%96%87%E5%8F%8A%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&#x2F;index.html">
<meta property="og:site_name" content="TianHongZXY">
<meta property="og:description" content="小样本学习 CLIP  一般的图像分类是把图片用backbone编码成一个feature vector，然后用一个classifier weight matrix W 乘上得到分类结果，$W_i$是class i的 prototype weight vector   f=\operatorname{Backbone}(\mathbf{I}), \quad p_{i}=\frac{\exp \lef">
<meta property="og:locale" content="en">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20211028205325488.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20211028204841604.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20211028160056618.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20211028160359546.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210910165248476.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210910165429797.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210910230306493.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210910233618116.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210508152345748.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210508160701738.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210514125147621.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210515213831523.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210907160525817.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;v2-65bf57c2d772d1f350c438297bdb7b02_1440w.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210906203320349.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;7B8A514D-8097-4093-8809-6FFD15C6AF1F.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;F8027122-1EB9-4E8C-9E6C-48BAB42C6256.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210905215922771.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210905215246448.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210819010348789.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210810232326256.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20210810015807326.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;0C37633B-16E3-483E-99B9-3518D6C9B6AD.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;948A9206-34B0-4EAB-8DDE-3DC9B5C1B9DE.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;738686F5-7511-4722-8485-CA228F13C2FA.png">
<meta property="og:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;20180519100022431.jpeg">
<meta property="og:updated_time" content="2021-11-04T09:18:24.654Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;gitee.com&#x2F;TianHongZXY&#x2F;blog-images&#x2F;raw&#x2F;master&#x2F;image-20211028205325488.png">






  <link rel="canonical" href="https://TianHongZXY.github.io/2021/05/08/论文及博客阅读笔记/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>论文及博客阅读笔记（持续更新） | TianHongZXY</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">TianHongZXY</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">那时我们一无所有，也没有什么能妨碍我们享受静夜</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br/>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://TianHongZXY.github.io/2021/05/08/%E8%AE%BA%E6%96%87%E5%8F%8A%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="TianHongZXY"/>
      <meta itemprop="description" content="浪漫骑士 行吟诗人 自由思想者"/>
      <meta itemprop="image" content="/images/header.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TianHongZXY"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">论文及博客阅读笔记（持续更新）

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2021-05-08 15:05:07" itemprop="dateCreated datePublished" datetime="2021-05-08T15:05:07+08:00">2021-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2021-11-04 17:18:24" itemprop="dateModified" datetime="2021-11-04T17:18:24+08:00">2021-11-04</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deep-learning/" itemprop="url" rel="index"><span itemprop="name">deep learning</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/05/08/%E8%AE%BA%E6%96%87%E5%8F%8A%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">Comments: </span> <span class="post-comments-count valine-comment-count" data-xid="/2021/05/08/%E8%AE%BA%E6%96%87%E5%8F%8A%E5%8D%9A%E5%AE%A2%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="小样本学习"><a href="#小样本学习" class="headerlink" title="小样本学习"></a>小样本学习</h2><ul>
<li><p>CLIP</p>
<ul>
<li><p>一般的图像分类是把图片用backbone编码成一个feature vector，然后用一个classifier weight matrix W 乘上得到分类结果，$W_i$是class i的 prototype weight vector</p>
</li>
<li><script type="math/tex; mode=display">
f=\operatorname{Backbone}(\mathbf{I}), \quad p_{i}=\frac{\exp \left(\mathbf{W}_{i}^{T} f\right) / \tau}{\sum_{j=1}^{N} \exp \left(\mathbf{W}_{j}^{T} f\right) / \tau}</script></li>
<li><p>CLIP的 zero shot 迁移范式是把visual backbone 和 text encoder 在大规模有噪音的数据集上使用对比学习预训练之后，不 fine tune 直接拿来作图像分类，给定K个图像类别，包括它们的名字(natural language name) ${C_1, C_2, …, C_k}$，CLIP把 name 填到预先定义好的 prompt 模板 H 里，然后用 text encoder 编码得到 classifier weight $W_i$，</p>
</li>
<li><script type="math/tex; mode=display">
\mathbf{W}_{i}=\operatorname{BERT}\left(\operatorname{Tokenizer}\left(\left[H ; C_{i}\right]\right)\right)</script></li>
</ul>
</li>
<li><p>CoPo, Learning to prompt for vision- language models, <a href="https://arxiv.org/abs/2109.01134" target="_blank" rel="noopener">arXiv</a></p>
<ul>
<li><p>与CLIP的区别在于没有用手工定义的prompt，而是用了一个 continuous prompt，它创建了L个随机初始化的 soft tokens，每个D维，然后用S和class name concat起来经过bert得到W</p>
</li>
<li><script type="math/tex; mode=display">
\mathbf{W}_{i}=\operatorname{BERT}\left(S;\operatorname{Tokenizer}\left( C_{i}\right)\right)</script></li>
</ul>
</li>
<li><p><strong>CLIP-Adapter: Better Vision-Language Models with Feature Adapters</strong>, <a href="https://arxiv.org/abs/2110.04544" target="_blank" rel="noopener">arXiv</a> ✅</p>
<ul>
<li><p>直接对feature和W做变换得到task-specific feature，并且使用门控残差连接原本的信息</p>
</li>
<li><script type="math/tex; mode=display">
\begin{array}{r}
A_{v}(f)=\operatorname{ReLU}\left(f^{T} \mathbf{W}_{1}^{v}\right) \mathbf{W}_{2}^{v} \\
A_{t}(\mathbf{W})=\operatorname{ReLU}\left(\mathbf{W}^{T} \mathbf{W}_{1}^{t}\right) \mathbf{W}_{2}^{t} \\
f^{\star}=\alpha A_{v}(f)^{T}+(1-\alpha) f \\
\mathbf{W}^{\star}=\beta A_{t}(\mathbf{W})^{T}+(1-\beta) \mathbf{W}
\end{array}</script></li>
<li><p>作者说如果下游任务和预训练用的数据集之间的semantic gap比较大，那么就需要更多的task-specific信息，即让$\alpha$大一点，选取更多的新feature，反之相反，如果$\alpha=0$效果并不好，即完全使用新的feature，原因作者认为是过拟合</p>
</li>
<li><p>作者说单纯用visual adapter要比单纯用text adapter或一起用要好，因为visual features在预训练和fine tuning阶段 semantic gap更大</p>
</li>
</ul>
</li>
</ul>
<h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><hr>
<h3 id="任务型对话"><a href="#任务型对话" class="headerlink" title="任务型对话"></a>任务型对话</h3><hr>
<ul>
<li><strong>[2021-EMNLP] Effective Sequence-to-Sequence Dialogue State Tracking</strong>, <a href="https://arxiv.org/abs/2108.13990" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>作者提了几个观点：1.Masked span prediction比自回归语言模型的预训练方式更适合迁移到DST任务 2. 在文本摘要任务上预训练可以对迁移到DST任务有帮助，作者用了Pegasus中提出的Gap Sentence Prediction作为目标在文本摘要任务上预训练 3. 对于Seq2Seq模型，输入full history效果更好，这点和 <strong>[2021-ACL] Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking</strong> 中提出的观点相反。<strong><em>缺点：没有开源！！！</em></strong></li>
</ul>
</li>
<li><strong>[2021-ACL] Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking</strong>, <a href="https://arxiv.org/abs/2107.12578" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li></li>
</ul>
</li>
</ul>
<h4 id="小样本"><a href="#小样本" class="headerlink" title="小样本"></a>小样本</h4><ul>
<li><strong>[2021-NAACL] Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State Tracking</strong>, <a href="https://arxiv.org/abs/2105.04222" target="_blank" rel="noopener">arXiv</a> <a href="https://github.com/facebookresearch/Zero-Shot-DST" target="_blank" rel="noopener">github</a> ✅<ul>
<li>作者对不同slot进行了分类，然后提出使用 Slot Type Informed Description 来捕捉不同 slots 之间共享的信息，把context和 slot description concat到一起输入encoder-decoder生成value，作者用不同的 slot description 进行了实验，其所提出的Slot Type 形式如下 <code>[prefix] [slot type] of [slot] of the domain</code>，比如<code>slot=hotel-stars</code>，则<code>slot description=numbers of stars of the hotel</code>，作者认为 explicit information about the target value (i.e., slot type) is important in the low resource condition，这种 type 知识可以在不同slot之间迁移</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20211028205325488.png" style="zoom:33%;" /></li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20211028204841604.png" style="zoom:33%;" /></li>
</ul>
</li>
<li><strong>[EMNLP-2021] Zero-Shot Dialogue State Tracking via Cross-Task Transfer</strong>, <a href="https://arxiv.org/pdf/2109.04655.pdf" target="_blank" rel="noopener">arXiv</a> <a href="https://github.com/facebookresearch/Zero-Shot-DST" target="_blank" rel="noopener">github</a> ✅<ul>
<li>作者的动机是说在QA中，每一个question都可以被认为是一个待填的slot，而DST数据集的slots非常有限，multiwoz有8000+对话，110000+轮，但是只有30个不到的slots，这样的话Zero-Shot时很难 generalize 到其他类型的 slot，而QA数据集有50w+问题，可以提高泛化能力</li>
<li>用QA任务训练模型，QA分为 Extrative Question 和 Multi-Choice Question，用 encoder-decoder 架构生成 answer，在DST Zero Shot阶段把所有slot建模为一个问题，如 “what is the <slot> of the <domain> that user wants?”</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20211028160056618.png" style="zoom:33%;" /></li>
<li>为了让模型学会识别unanswerable question（给slot预测none），设计了两种方式构建负样本，一种是Negative Question Sampling，即随机采样出和Context无关的Question，这种Out-of-Context的QA对应Out-of-Domain的DST，另一种是Context Truncation, 即把QA中包含answer的Context给截掉，这种针对DST中的In-Domain-Unmentioned，但是消融实验表明CT的帮助并不大，而NQS却影响非常大，作者解释NQS有效是因为unanswerable问题太多了，所以在QA中加这个和DST任务的gap比较小</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20211028160359546.png" style="zoom:33%;" /></li>
<li><strong>我的思考: CT和NQS都是属于unanswerable，但是NQS采样的问题都是和context无关的，CT只是截掉了答案，为什么CT效果不是更好呢？它毕竟是Context-Related的，相比NQS更不容易分辨出是unanswerable</strong></li>
<li>作者通过 error analysis 发现79.79%的错误来自于slot gate prediction(whether the slot is unanswerable or not)，只有20.21%的错误是 slot value 预测错了，两种典型错误是 slot value 还没被 user 确定就被模型填上了，另一种是 user 已经提到了slot value 而模型没有捕捉到，作者的Oracle Study也表明了 slot gate prediction accuracy 非常重要，zero shot on multiwoz 能从35.77%提到56.06%</li>
</ul>
</li>
</ul>
<h3 id="开放域对话"><a href="#开放域对话" class="headerlink" title="开放域对话"></a>开放域对话</h3><hr>
<ul>
<li><strong>Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation</strong>, <strong>ACM MM, 2021</strong>, <a href="https://arxiv.org/abs/2109.05778" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>2021-10-10组会报告</li>
</ul>
</li>
<li><strong>Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/abs/2105.15171" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>扰乱context X 得到 X’，然后 condition on X 和 X’ 分别生成回复 Y 和 Y’，Y 的概率相比 Y’ 越大越好，没什么意思，不知道这怎么都能中</li>
</ul>
</li>
<li><strong>Towards Quantifiable Dialogue Coherence Evaluation</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/pdf/2106.00507.pdf" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>提出了一个新的 dialogue coherence 评估模型，用了一个多层次的打分loss（3个level，level越高的response越coherent，模型给分也要越高），加上知识蒸馏和对其的正则loss防止fine-tune时灾难性遗忘，实验效果显示相比于其他自动评估指标，跟人工标注的打分相关系数更高。</li>
</ul>
</li>
<li><strong>I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/abs/2012.13391" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>提出了一个新的数据集来做dialogue contradiction detection，比用NLI数据集好很多；提了一个structured utterance-based approach来检测对话冲突，就是把要检测的这句话和该speaker之前说过的话组成pair，计算分数，取最高分的那个做输出。</li>
</ul>
</li>
<li><strong>NEURAL GENERATION OF OPEN-ENDED TEXT AND DIALOGUE</strong>, <strong>Abigail See PhD Thesis, 2021</strong>, <a href="https://purl.stanford.edu/hw190jq4736" target="_blank" rel="noopener">PDF</a> ✅</li>
<li><strong>BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/abs/2106.06169" target="_blank" rel="noopener">arXiv</a> ✅</li>
<li><strong>Towards Emotional Support Dialog Systems</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/abs/2106.01144" target="_blank" rel="noopener">arXiv</a> ✅</li>
<li><strong>Towards a Human-like Open-Domain Chatbot</strong>, <strong>Google Brain, 2020</strong>, <a href="https://arxiv.org/abs/2001.09977" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>提出了一个人工评估指标叫 Sensiblenes Specificity Average(SSA)，实验显示SSA和Perplexity之间有很强正相关，两个我觉得作者提出的比较重要的观点：1. beam-search decoding 会生成重复和无趣的回复，当模型的困惑度足够低时，单纯地使用 sample-and-rerank 就已经足够生成多样的、内容丰富的回复，sample-and-rerank 指在温度T下采样N个回复，然后根据生成的概率 rerank， 选择概率最高的那个回复；2. 在非常大的社交语料上的 perplexity 可以作为一个人工评估指标（human-likeness, sensibleness and specificity）的代表，这意味着仅仅需要最小化大规模社交语料上的 perplexity 就可以获得一个人性化的开放域对话模型</li>
</ul>
</li>
<li><strong>Recipes for building an open-domain chatbot</strong>, <strong>FAIR, 2020</strong>, <a href="https://arxiv.org/abs/2004.13637" target="_blank" rel="noopener">arXiv</a></li>
</ul>
<h3 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h3><hr>
<ul>
<li><strong>xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering</strong>, <strong>ACL, 2021</strong>, <a href="https://aclanthology.org/2021.acl-long.477.pdf" target="_blank" rel="noopener">PDF</a> ✅<ul>
<li>使用了两套fast和slow encoder，分别对问题和文章编码，然后优化fast question与slow passage，和slow question和fast passge，fast encoder编码的question与passage不直接互相影响，而是通过对方的slow encoder影响，而slow encoder依赖对应的fast encoder进行更新</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210910165248476.png" alt="image-20210910165248476" style="zoom:33%;" /></li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210910165429797.png" alt="image-20210910165429797" style="zoom:45%;" /></li>
</ul>
</li>
<li><strong>A Semantic-based Method for Unsupervised Commonsense Question Answering</strong>, <strong>ACL, 2021</strong>, <a href="https://arxiv.org/abs/2105.14781" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>提出了一个基于语义的无监督QA方法，不同于之前的方法利用预训练语言模型去计算$P(answer ~choice \vert question)$，因为作者认为生成answer的概率很容易被一些因素影响，比如词频、句子结构（比如换个同义词或者换成被动句式打分就低了），所以作者希望是基于answer的语义，而不是answer这句话本身，作者先通过rewrite的方式，让PLM基于question生成多个回复，这些回复叫做supporters，然后用一个句子语义相似度模型，比如SBERT等，针对每一个answer choice，计算所有supporters和它的相似度并求和，得到该answer choice的得分，最终得分最高的answer choice被选择。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210910230306493.png" alt="image-20210910230306493" style="zoom: 50%;" /></li>
</ul>
</li>
</ul>
<h3 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h3><hr>
<ul>
<li>A Survey of Transformers <a href="https://arxiv.org/abs/2106.04554" target="_blank" rel="noopener">arXiv</a></li>
<li><strong>Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling</strong>, <strong>ACL, 2021, short</strong>, <a href="https://arxiv.org/abs/2106.01040" target="_blank" rel="noopener">arXiv</a> ✅<ul>
<li>第一步：先用类sbert模型获得文档中每个句子的句子表征([CLS]的embedding)，第二步：再把句子表征当word，经过一个transformer得到看到整个文档全局信息的表征，第三步：把第二步的表征当成第一步的[CLS]再来一次，得到最终的表征，进行Hierarchical Pooling得到最终的文档表征。个人认为是刮痧。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210910233618116.png" alt="image-20210910233618116" style="zoom:33%;" /></li>
</ul>
</li>
</ul>
<h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h3><hr>
<ul>
<li><p>#TODO <strong>Contrastive Learning with Adversarial Perturbations for Conditional Text Generation</strong>, <strong>ICLR, 2021</strong></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/400444415" target="_blank" rel="noopener">利用对比学习缓解文本生成中的曝光偏差问题</a>, 文章探索了一种利用对比学习来缓解曝光偏差的方法，借助梯度，在表示空间对解码端表示进行修改，从而生成高难度的正例和负例用于对比学习</li>
</ul>
</li>
<li><p><strong>Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models</strong>, Ashwin K Vijayakumar et al. <strong>arxiv, 2016</strong> <a href="/Users/tianhongzxy/Documents/论文及代码/NLP方向/NLP论文/已读/Diverse Beam Search-Decoding Diverse Solutions from Neural Sequence Models.pdf">PDF</a> <a href="https://arxiv.org/abs/1610.02424" target="_blank" rel="noopener">arXiv</a> (Citations <strong>219</strong>) ✅</p>
<ul>
<li><p>提出diverse beam search，候选句的分数不但与概率有关，还与其他已经生成的句子的相似度有关。<a href="https://blog.csdn.net/YUFAN_ZHAO/article/details/79132898" target="_blank" rel="noopener">解读博客</a></p>
<p><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210508152345748.png" alt="image-20210508152345748" style="zoom: 33%;" /></p>
</li>
</ul>
</li>
<li><p><strong>Target Conditioning for One-to-Many Generation</strong>, Marie-Anne Lachaux et al. <strong>EMNLP-findings, 2020</strong> <a href="/Users/tianhongzxy/Documents/论文及代码/NLP方向/NLP论文/已读/Target Conditioning for One-to-Many Generation.pdf">PDF</a> <a href="https://arxiv.org/abs/2009.09758" target="_blank" rel="noopener">arXiv</a> (Citations <strong>0</strong>) ✅</p>
<ul>
<li><p>这篇工作借鉴了 discrete autoencoders 的思路，提出将一个 discrete target encoder 引入到翻译模型中，方便将每一个目标语句关联到对应的 variable 或者 domain。其中每一个 domain 对应一个 embedding，这样在测试阶段可以根据每个 domain embedding 来生成多样性的翻译。并且这种离散化的表示方式允许无监督地方式来改变翻译的 domain 信息。<strong>周报2021.04.25中解读</strong></p>
<p><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210508160701738.png" alt="image-20210508160701738" style="zoom: 25%;" /></p>
</li>
</ul>
</li>
<li><p><a href="https://spaces.ac.cn/archives/7259" target="_blank" rel="noopener">Seq2Seq中Exposure Bias现象的浅析与对策</a> by 苏剑林 ✅</p>
<ul>
<li>缓解teacher forcing造成的exposure bias问题，作者提出了两个简单的方法：1. 构建负样本，从target中随机抽取单词并替换decoder使用teacher forcing时的输入词。2. 对抗训练（梯度惩罚）</li>
</ul>
</li>
<li><p><a href="https://spaces.ac.cn/archives/6933" target="_blank" rel="noopener">从语言模型到Seq2Seq：Transformer如戏，全靠Mask</a> by 苏剑林 ✅</p>
<ul>
<li>在Transformer中利用不同的attention mask 实现乱序语言模型，论文<a href="https://arxiv.org/abs/1905.02450" target="_blank" rel="noopener">MASS</a>和<a href="https://arxiv.org/abs/1905.03197" target="_blank" rel="noopener">UNILM</a></li>
</ul>
</li>
<li><p><strong>Data Distillation for Controlling Specificity in Dialogue Generation</strong>, Jiwei Li et al. <strong>arxiv, 2017</strong> <a href="https://arxiv.org/pdf/1702.06703.pdf" target="_blank" rel="noopener">PDF</a> <a href="https://arxiv.org/abs/1702.06703" target="_blank" rel="noopener">arXiv</a> (Citations <strong>18</strong>) ✅</p>
<ul>
<li><p>每轮把模型生成的最频繁的句子选出来，然后和训练集的句子计算相似度，去掉这些相似度最高的句子</p>
</li>
<li><div align=center>
  <img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210514125147621.png" alt="image-20210514125147621" style="zoom: 40%;" />
</div>
</li>
</ul>
</li>
<li><p><strong>Focus-Constrained Attention Mechanism for CVAE-based Response Generation</strong>, Zhi Cui et al. <strong>EMNLP-findings, 2020</strong> <a href="/Users/tianhongzxy/Documents/论文及代码/NLP方向/NLP论文/已读/Focus-Constrained Attention Mechanism for CVAE-based Response Generation.pdf">PDF</a> <a href="https://arxiv.org/abs/2009.12102" target="_blank" rel="noopener">arXiv</a> (Citations <strong>0</strong>) ✅</p>
<ul>
<li>把隐变量z和encoder output做attention得到focus，然后把focus和encoder output concat起来输入decoder，并在每一步解码求attention时使用；引入coverage vector，本质是解码时每一步的attention权重累加；设计focus constraint，本质是让focus和coverage vector欧几里得距离最小。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210515213831523.png" alt="image-20210515213831523" style="zoom: 20%;" /></li>
</ul>
</li>
<li><p><a href="https://www.aclweb.org/anthology/P17-1061.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P17-1061.pdf</a> Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1606.07947.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.07947.pdf</a> Sequence-Level Knowledge Distillation</p>
</li>
</ul>
<h3 id="词表征"><a href="#词表征" class="headerlink" title="词表征"></a>词表征</h3><hr>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/50443871" target="_blank" rel="noopener">NLP的巨人肩膀</a>，梳理了从2003年的NNLM开始，到今天的BERT、GPT这些语言表征学习的发展历程，探究NLP或NLU的历史，也可以说同样也是探究文本如何更有效表征的历史。深度好文，推荐！✅</li>
</ul>
<h4 id="有监督"><a href="#有监督" class="headerlink" title="有监督"></a>有监督</h4><ul>
<li><strong>Learned in Translation: Contextualized Word Vectors</strong>, 2017, <a href="https://arxiv.org/abs/1708.00107" target="_blank" rel="noopener">arXiv</a>, (Citations <strong>750</strong>)<ul>
<li>CoVe，算是ELMo的前辈，首先用一个Encoder-Decoder框架在机器翻译的训练语料上进行预训练，之后用训练好的模型，只取其中的Embedding层和Encoder层，同时在一个新的任务上设计一个task-specific模型，然后将原先预训练好的Embedding层和Encoder层的输出作为这个task-specific模型的输入，最终在新的任务场景下进行训练。和诸如Skip-Thoughts等方法有所不同的是，CoVe更侧重于如何将现有数据上预训练得到的表征迁移到新任务场景中，而之前的句子级任务中大多数都只把迁移过程当做一个评估他们表征效果的手段（比如STS和SentEval），因此观念上有所不同。</li>
</ul>
</li>
</ul>
<h4 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h4><ul>
<li><strong>Deep contextualized word representations</strong>, <strong>NAACL, 2018</strong>, <a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">arXiv</a>, (Citations <strong>7706</strong>)<ul>
<li>ELMo借鉴了2016年Google Brain的Rafal Jozefowicz等人发表的一篇论文《Exploring the Limits  of Language Modeling》，其主要改进在于输入层和输出层不再是word，而是变为了一个 char-based CNN 结构，ELMo在输入层和输出层考虑了使用同样的这种结构，这样不用再维护一个V*h大的word embedding，还可以解决OOV问题，只需要维护一个26*h的char embedding和CNN，CNN对所有单词都是共享参数的，所以大大减少了参数量，在预测阶段，CNN对于每一个词向量的计算可以预先做好（就是把所有单词的表征CNN(t_k)都给计算出来），更能够减轻inference阶段的计算压力</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210907160525817.png" alt="image-20210907160525817" style="zoom: 33%;" /></li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/v2-65bf57c2d772d1f350c438297bdb7b02_1440w.jpg" alt="img" style="zoom: 25%;" /></li>
</ul>
</li>
<li><strong>GloVe</strong>，$X_{ij}$是两个词i和j在某个窗口大小中的共现频率，是一个权重系数，主要目的是共现越多的pair对于目标函数贡献应该越大，但是又不能无限制增大，所以对共现频率过于大的pair限定最大值，以防训练的时候被这些频率过大的pair主导了整个目标函数，两个b值是两个偏置项，那么剩下的$\left(w_{i}^{T} w_{j}-\log X_{i j}\right)^{2}$其实就是一个普通的均方误差函数，$w_i$ 是当前词的向量，$w_j$对应的是与其在同一个窗口中出现的共现词的词向量，两者的向量点乘要去尽量拟合它们共现频率的对数值。从直观上理解，如果两个词共现频率越高，那么其对数值当然也越高，因而算法要求二者词向量的点乘也越大，而二个词向量的点乘越大，其实包含了两层含义：第一，要求各自词向量的模越大，通常来说，除去频率非常高的词（比如停用词)，对于有明确语义的词来说，它们的词向量模长会随着词频增大而增大（因为词频高所以概率score大，所以应该模长大），因此两个词共现频率越大，要求各自词向量模长越大是有直觉意义的，比如“魑魅魍魉”假如能被拆分成两个词，那么“魑魅”和“魍魉”这两个词的共现频率相比““魑魅”和其他词的共现频率要大得多，对应到“魑魅”的词向量，便会倾向于在某个词向量维度上持续更新，进而使得它的模长也会比较偏大；第二，要求这两个词向量的夹角越小，这也是符合直觉的，因为出现在同一个语境下频率越大，说明这两个词的语义越接近，因而词向量的夹角也偏向于越小。（GloVe考虑到了对距离较远的词对做相应的惩罚，这一点是怎么做的还未了解，距离远是指按字典序排离得很远吗？）<ul>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210906203320349.png" alt="image-20210906203320349" style="zoom:33%;" /></li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/7B8A514D-8097-4093-8809-6FFD15C6AF1F.png" alt="7B8A514D-8097-4093-8809-6FFD15C6AF1F" style="zoom:33%;" /></li>
</ul>
</li>
</ul>
<h3 id="句子表征"><a href="#句子表征" class="headerlink" title="句子表征"></a>句子表征</h3><hr>
<h4 id="有监督-1"><a href="#有监督-1" class="headerlink" title="有监督"></a>有监督</h4><ul>
<li><strong>Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</strong>, <strong>ICLR 2018</strong>, <a href="https://arxiv.org/abs/1804.00079" target="_blank" rel="noopener">arXiv</a> (Citations <strong>264</strong>)<ul>
<li>提出了利用四种不同的监督任务来联合学习句子的表征，这四种任务分别是：Natural Language Inference, Skip-thougts, Neural Machine Translation 以及 Constituency Parsing。作者认为，通用的句子表征应该通过侧重点不同的任务来联合学习到，而不是只有一个特定任务来学习句子表征。所以模型将同时在<strong>多个任务</strong>和<strong>多个数据源</strong>上进行训练，并且<strong>共享句子表征</strong>。先用联合学习的方法在上述四个任务上进行训练，训练结束后，将模型参数冻结，只作为特征提取器提取句子表征，然后直接接上全连接层作为分类器，在新的分类任务上只训练分类器。作者发现很多任务上简单分类器都要超过当时的最好结果，并且他们还发现联合训练中不同的任务对于句子表征中的不同方面有不同的贡献。</li>
</ul>
</li>
<li><strong>Parameter-free Sentence Embedding via Orthogonal Basis</strong>, <strong>EMNLP, 2019</strong>, <a href="https://arxiv.org/abs/1810.00438" target="_blank" rel="noopener">arXiv</a> (Citations <strong>15</strong>) ✅</li>
<li><strong>SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models</strong>, 2020, <a href="https://arxiv.org/abs/2002.06652" target="_blank" rel="noopener">arXiv</a> (Citations <strong>30</strong>) ✅<ul>
<li>上一篇论文是用在静态词向量，这篇用在BERT这种contextual embedding</li>
</ul>
</li>
<li><strong>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</strong>, <strong>EMNLP, 2018</strong>, <a href="https://arxiv.org/abs/1705.02364" target="_blank" rel="noopener">arXiv</a> (Citations <strong>1418</strong>) ✅</li>
<li><strong>Universal Sentence Encoder</strong>, <strong>EMNLP, 2018</strong> <a href="https://arxiv.org/abs/1803.11175" target="_blank" rel="noopener">arXiv</a> (Citations <strong>499</strong>) ✅<ul>
<li>综合利用<strong>无监督训练数据和有监督训练数据</strong>，进行<strong>多任务训练</strong>，从而学习一个通用的句子编码器。无监督训练数据包括问答(QA)、维基百科和网页新闻等，有监督训练数据为SNLI。多任务模型设计如下图所示，其中灰色的 encoder 为共享参数的句子编码器。<strong>共享编码器</strong>使得模型训练时间大大减少，同时还能保证各类迁移学习任务的性能，力求为尽可能多的应用提供一种通用的句子编码器。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/F8027122-1EB9-4E8C-9E6C-48BAB42C6256.png" alt="F8027122-1EB9-4E8C-9E6C-48BAB42C6256" style="zoom:33%;" /></li>
</ul>
</li>
</ul>
<h4 id="无监督-1"><a href="#无监督-1" class="headerlink" title="无监督"></a>无监督</h4><ul>
<li><p>#TODO ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</p>
</li>
<li><p>#TODO Smoothed Contrastive Learning for Unsupervised Sentence Embedding</p>
</li>
<li><p><strong>An efficient framework for learning sentence representations</strong>, <strong>ICLR, 2018</strong>, <a href="https://arxiv.org/abs/1803.02893" target="_blank" rel="noopener">arXiv</a>, (Citations <strong>291</strong>) ✅</p>
<ul>
<li>Use the meaning of the current sentence to predict the meanings of adjacent sentences, where meaning is represented by an embedding of the sentence computed from an encoding function. Viewing generation as choosing a sentence from all possible sentences, this can be seen as a discriminative approximation to the generation problem. At test time, for a given sentence s, we consider its representation to be the concatenation of the outputs of the two encoders [f(s) g(s)]. 用给定的句子去预测候选的一组句子是否为其的毗邻句，把生成任务转为从所有可能的句子里选出真实的句子，用判别的方式去近似生成问题。很有对比学习的感觉，测试时用两个encoder f和g的concat作为句子向量</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210905215922771.png" alt="image-20210905215922771" style="zoom:33%;" /></li>
</ul>
</li>
<li><p><strong>A Simple but Tough-to-Beat Baseline for Sentence Embeddings</strong>, <strong>ICLR, 2017</strong>, <a href="https://openreview.net/pdf?id=SyK00v5xx" target="_blank" rel="noopener">OpenReview</a> (Citations <strong>948</strong>) ✅</p>
<ul>
<li>本质是一个词袋模型，以平滑倒词频 (smooth inverse frequency, SIF) 作为权重对词向量进行加权平均得到句子向量，即认为频率越低的词在在句子中的重要性更大。最后令每个句子向量都减去其在所有句子向量组成的矩阵的第一个主成分上的投影，即抹去所有句子的共有信息，这样可以增大每个句子向量之间的距离。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210905215246448.png" alt="image-20210905215246448" style="zoom:33%;" /></li>
</ul>
</li>
<li><p><strong>Bootstrapped Unsupervised Sentence Representation Learning</strong>, Yan Zhang, <strong>ACL, 2021</strong>, <a href="https://aclanthology.org/2021.acl-long.402/" target="_blank" rel="noopener">ACL</a>, <a href="https://github.com/yanzhangnlp/BSL" target="_blank" rel="noopener">github还没开源</a> ✅</p>
<ul>
<li>把BYOL那一套照抄到了NLP领域</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210819010348789.png" alt="image-20210819010348789" style="zoom:33%;" /></li>
</ul>
</li>
<li><p><strong>An Unsupervised Sentence Embedding Method by Mutual Information Maximization</strong>, Yan Zhang, <strong>EMNLP, 2020</strong>, <a href="https://arxiv.org/abs/2009.12061" target="_blank" rel="noopener">arXiv</a>, <a href="https://github.com/yanzhangnlp/IS-BERT" target="_blank" rel="noopener">github</a> (Citations <strong>10</strong>) ✅</p>
<ul>
<li><p>在BERT上面堆几个window size不同(1,3,5)的1D-CNN，然后把不同window size的特征concat起来作为token-level local representation $\mathcal{F}_{\theta}^{(i)}(\mathbf{x})$，对句子里所有的token表征做pooling后得到sentence-level global representation $\mathcal{E}_{\theta}(\mathbf{x})$，最大化同一句子全局表征和局部表征的琴生-香侬估计(Jensen-Shannon MI estimator)，如下式，sp是softplus函数$sp(z)=\log(1+e^z)$​，性能在STS和SentEval task是当时无监督方法里最好的，媲美有监督的InferSent，但不如USE和SBERT，优点是不需要标注数据，可以在各种task-specific的数据集上训练，指出了SBERT这类有监督模型在NLI这种非task-specific数据集上训练后，迁移到其他domain的数据集上时不适应，效果非常差</p>
</li>
<li><p><strong>#TODO 如果把顶层的CNN换成现在的vision transformer，会不会更好？</strong></p>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
&\widehat{\mathcal{I}}_{\omega}^{J S D}\left(\mathcal{F}_{\theta}^{(i)}(\mathbf{x}) ; \mathcal{E}_{\theta}(\mathbf{x})\right):= \\
&\quad E_{\mathbb{P}}\left[-s p\left(-T_{\omega}\left(\mathcal{F}_{\theta}^{(i)}(\mathbf{x}), \mathcal{E}_{\theta}(\mathbf{x})\right)\right)\right] \\
&\quad-E_{\mathbb{P} \times \tilde{\mathbb{P}}}\left[s p\left(T_{\omega}\left(\mathcal{F}_{\theta}^{(i)}\left(\mathbf{x}^{\prime}\right), \mathcal{E}_{\theta}(\mathbf{x})\right)\right)\right]
\end{aligned}</script></li>
<li><p><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210810232326256.png" alt="image-20210810232326256" style="zoom:33%;" /></p>
</li>
</ul>
</li>
<li><p><strong>Self-training Improves Pre-training for Natural Language Understanding</strong>, <strong>NAACL, 2021</strong>, <a href="https://arxiv.org/abs/2010.02194" target="_blank" rel="noopener">arXiv</a> (Citations <strong>25</strong>) ✅</p>
<ul>
<li>准备一个非常非常大的句子集，包含上亿个的句子，然后用一个训练好的句子编码器（这个句子编码器是为语义相似度搜索专门训练的，用的triplet loss）去编码下游任务的训练集，<strong>根据class分类，把同一class的句子表征取平均作为query，然后到句子集里召回最相似的句子</strong>，把这些句子给一个在下游任务上fine-tune过的大型预训练teacher模型打上软标签，作为新的in-domain task-specific的人造数据集，最终用一个参数量更少的student模型在这些人造数据集上训练得到最终模型，<strong>这种self-training方式可以作为pre-training的补充，能进一步提升性能</strong>，总体上用到了句子表征编码器、知识蒸馏，<strong>github有人问作者这本质和远程监督有什么区别，未获得回复</strong></li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/image-20210810015807326.png" alt="SentAugment" style="zoom: 25%;" /></li>
</ul>
</li>
<li><p><strong>Skip-Thought Vectors</strong>, <strong>NIPS 2015</strong> <a href="https://arxiv.org/abs/1506.06726" target="_blank" rel="noopener">arXiv</a> (Citations <strong>2232</strong>) ✅</p>
<ul>
<li>用一个基本的encoder-decoder模型，使用小说集作为语料，假设有三句连贯的句子，用编码器把中间那句编码为一个向量，然后用两个解码器分别解码生成它的上句和下句。使用时冻结参数，用编码器获得句子表征，然后在它上面加个线性分类器，就可以训练后做分类了</li>
<li>词表扩充，可以使用一个大型的预训练词向量，文中用了CBOW，用一个矩阵W把CBOW的embedding映射到模型的embedding，最小化L2损失得到W，之后在使用中遇到训练时没有的单词就用W对其进行映射</li>
</ul>
</li>
<li><p><strong>CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding</strong>, Dong Wang et al. <strong>ACL, 2021</strong> <a href="https://arxiv.org/abs/2107.00440" target="_blank" rel="noopener">arXiv</a> (Citations <strong>0</strong>) ✅</p>
</li>
<li><p><a href="https://amitness.com/2020/05/self-supervised-learning-nlp/" target="_blank" rel="noopener">Self Supervised Representation Learning in NLP</a> 总结了11种nlp任务用到的自监督方法，比如CBOW、skip-gram、MLM、NSP等等 ✅</p>
</li>
<li><p><strong>Self-Guided Contrastive Learning for BERT Sentence Representations</strong>, Taeuk Kim et al. <strong>ACL, 2021</strong> <a href="https://arxiv.org/abs/2106.07345" target="_blank" rel="noopener">arXiv</a> (Citations <strong>0</strong>) ✅</p>
</li>
<li><p><strong>SimCSE: Simple Contrastive Learning of Sentence Embeddings</strong> <a href="https://arxiv.org/abs/2104.08821" target="_blank" rel="noopener">arXiv</a> <a href="https://github.com/princeton-nlp/SimCSE" target="_blank" rel="noopener">github</a> (Citations <strong>3</strong>) ✅</p>
<ul>
<li>目前semantic textual similarity tasks的 SOTA，支持无监督和有监督，使用对比学习，正例对只使用了两个不同的dropout mask</li>
</ul>
</li>
<li><p><strong>DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</strong> <strong>ACL,2021</strong>, <a href="https://arxiv.org/abs/2006.03659" target="_blank" rel="noopener">arXiv</a> <a href="https://github.com/JohnGiorgi/DeCLUTR" target="_blank" rel="noopener">github</a> (Citations <strong>23</strong>) ✅</p>
<ul>
<li>对每一篇文档，随机采样几个span作为anchor samples，对于每个anchor sample从同一个文档里采样几个span作为positive samples(span的长度服从beta分布，anchor偏长，positive偏短)，用transformer模型分别编码它们并做pooling得到句子表征，把几个positive samples求平均得到anchor sample的正例表征，使用NT-Xent loss函数</li>
</ul>
</li>
<li><p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</p>
</li>
<li><p>Evaluating Models’ Local Decision Boundaries via Contrast Sets, AllenAI, EMNLP-findings 2020</p>
</li>
</ul>
<h2 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h2><hr>
<h3 id="图片表征"><a href="#图片表征" class="headerlink" title="图片表征"></a>图片表征</h3><hr>
<ul>
<li><strong>Learning deep representations by mutual information estimation and maximization</strong>, <strong>R Devon Hjelm, ICLR, 2019</strong>,  <a href="https://arxiv.org/abs/1808.06670" target="_blank" rel="noopener">arXiv</a>, <a href="https://github.com/rdevon/DIM" target="_blank" rel="noopener">github</a> (Citations <strong>864</strong>) ✅<ul>
<li><del>太难了没怎么看明白</del>，作者指出仅仅是最大化模型输入的特征与输出的表征之间的互信息是不够的，相反，如果最大化输出表征与输入的局部区域(local region)特征之间的平均互信息，能够极大地提升表征的质量。作者表明DIM可以使用不同的MI估计方法，文中使用了三种：基于 DV representation、琴生香侬估计JSD、InfoNCE，作者发现JSD对负样本的数量不敏感，几乎不受负样本数量的影响，infoNCE效果随负样本数量提升而提升，DV 受到负样本数量的影响最大，但是随着负样本数量的增加，它们之间的差距会逐渐缩小。</li>
<li><a href="https://kexue.fm/archives/6024" target="_blank" rel="noopener">深度学习的互信息：无监督提取特征 </a> by 苏剑林 ✅</li>
<li><a href="https://zhuanlan.zhihu.com/p/277660074" target="_blank" rel="noopener">对 Deep InfoMax（DIM）的理解</a> 知乎博客，大体是翻译了原论文加入作者的一些理解，翻译得比较好，global infomax和local infomax的流程也梳理得比较清晰 ✅</li>
<li><a href="https://zhuanlan.zhihu.com/p/48123360" target="_blank" rel="noopener">DIM：通过最大化互信息来学习深度表征</a> 知乎博客，作者从最大化互信息和先验KL散度约束的目标函数出发，中间将KL散度换成局部变分法推导的JS散度，最终推导出结果等价于NCE，内容比较像苏剑林的博客 ✅</li>
</ul>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/102573476" target="_blank" rel="noopener">无监督学习: Kaiming一作 动量对比(MoCO)论文笔记 </a> ✅</li>
<li><strong>Momentum Contrast for Unsupervised Visual Representation Learning</strong> <a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">arXiv</a> (Citations <strong>952</strong>)</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/0C37633B-16E3-483E-99B9-3518D6C9B6AD.png" alt="pytorch伪代码" style="zoom: 20%;" /></li>
<li><a href="https://zhuanlan.zhihu.com/p/107126866" target="_blank" rel="noopener">无监督学习距离监督学习还有多远？Hinton组新作解读</a> ✅</li>
</ul>
<ul>
<li>#TODO Learning deep representations by mutual information estimation and maximization</li>
<li>#TODO A theoretical analysis of contrastive unsupervised representation learning <a href="https://arxiv.org/abs/1902.09229" target="_blank" rel="noopener">arXiv</a></li>
<li>#TODO NIPS2020: What Makes for Good Views for Contrastive Learning?</li>
<li>#TODO ICLR2021: Self-Supervised Learning From a Multi-View Perspective</li>
<li>#TODO ICLR2021: FAIRFIL: Contrastive Neural Debiasing Method For Pretrained Text Encoders</li>
<li></li>
</ul>
<h2 id="重参数化"><a href="#重参数化" class="headerlink" title="重参数化"></a>重参数化</h2><hr>
<ul>
<li><strong>Categorical Reparameterization with Gumbel-Softmax</strong>, Eric Jang et al. <strong>ICLR, 2017</strong> <a href="/Users/tianhongzxy/Documents/论文及代码/NLP方向/NLP论文/已读/Categorical Reparameterization with Gumbel-Softmax.pdf">PDF</a> <a href="https://arxiv.org/abs/1611.01144" target="_blank" rel="noopener">arXiv</a> (Citations <strong>1881</strong>), <a href="https://blog.evjang.com/2016/11/tutorial-categorical-variational.html" target="_blank" rel="noopener">Jang’s blog</a> ✅</li>
<li><p>Gumbel Max和Gumbel Softmax详细推导参考<a href="https://kexue.fm/archives/6705" target="_blank" rel="noopener">漫谈重参数：从正态分布到Gumbel Softmax</a> by 苏剑林，<a href="https://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html" target="_blank" rel="noopener">英文博客</a>，重参数化的作用简易理解参考下面来自<a href="https://www.zhihu.com/question/62631725/answer/201338234" target="_blank" rel="noopener">知乎回答</a>的评论区中Towser的回复 ✅</p>
</li>
<li><p>#TODO <a href="https://arxiv.org/pdf/1308.3432.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1308.3432.pdf</a> Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</p>
</li>
<li>#TODO <a href="https://arxiv.org/abs/1611.00712" target="_blank" rel="noopener">https://arxiv.org/abs/1611.00712</a> The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</li>
</ul>
<h2 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h2><hr>
<ul>
<li><strong>Distilling the Knowledge in a Neural Network</strong>, Hinton et al. <strong>arxiv, 2015</strong> <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">PDF</a> <a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">arXiv</a> (Citations <strong>6051</strong>) ✅<ul>
<li>使用大模型的输出作为软标签，加上真实标签作为硬标签，一同求loss，指导student model，使用带温度的softmax，注意温度为$T$的softmax的loss需要乘以$T^2$，这是为了让损失函数的两项的梯度大致在一个数量级上，<a href="https://zhuanlan.zhihu.com/p/90049906" target="_blank" rel="noopener">解读博客1</a>，<a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">解读博客2</a></li>
</ul>
</li>
</ul>
<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><hr>
<ul>
<li><a href="https://kexue.fm/archives/6760" target="_blank" rel="noopener">VQ-VAE的简明介绍：量子化自编码器</a> ✅ 因为没看过pixel-cnn导致不能完全看懂，其中提到的stop gradient方法可以为很多函数自己定义梯度，实用价值需要具体任务具体分析，有参考启发意义。</li>
<li>#TODO <a href="https://blog.evjang.com/2016/08/variational-bayes.html" target="_blank" rel="noopener">vae-tutorial</a></li>
</ul>
<h2 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h2><hr>
<ul>
<li><a href="https://spaces.ac.cn/archives/7234" target="_blank" rel="noopener">对抗训练浅谈：意义、方法和思考</a> by 苏剑林 ✅<ul>
<li>快速梯度等价于梯度惩罚，快速梯度是给输入x加上$\Delta x=\epsilon \nabla_{x} L(x, y ; \theta)$，梯度惩罚是给loss加上$\frac{1}{2} \epsilon\left|\nabla_{x} L(x, y ; \theta)\right|^{2}$</li>
</ul>
</li>
<li><a href="https://fyubang.com/2019/10/15/adversarial-train" target="_blank" rel="noopener">功守道：NLP中的对抗训练 + PyTorch实现</a> by 富邦</li>
<li>#TODO <a href="https://arxiv.org/abs/1312.6199" target="_blank" rel="noopener">https://arxiv.org/abs/1312.6199</a> Intriguing properties of neural networks (Citations <strong>6912</strong>)</li>
<li>#TODO <a href="https://arxiv.org/abs/1706.06083" target="_blank" rel="noopener">https://arxiv.org/abs/1706.06083</a> Towards Deep Learning Models Resistant to Adversarial Attacks (Citations <strong>3192</strong>)</li>
<li>#TODO <a href="https://arxiv.org/abs/1412.6572" target="_blank" rel="noopener">https://arxiv.org/abs/1412.6572</a> Explaining and Harnessing Adversarial Examples (Citations <strong>7566</strong>)</li>
<li>#TODO <a href="https://arxiv.org/abs/1605.07725" target="_blank" rel="noopener">https://arxiv.org/abs/1605.07725</a> Adversarial Training Methods for Semi-Supervised Text Classification (Citations <strong>439</strong>)</li>
<li>#TODO <a href="https://arxiv.org/abs/1711.09404" target="_blank" rel="noopener">https://arxiv.org/abs/1711.09404</a> Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients (Citations <strong>278</strong>)</li>
<li></li>
</ul>
<h2 id="数学相关"><a href="#数学相关" class="headerlink" title="数学相关"></a>数学相关</h2><hr>
<ul>
<li><p>#TODO <a href="https://kexue.fm/archives/8679" target="_blank" rel="noopener">《让人惊叹的Johnson–Lindenstrauss引理：理论篇》</a></p>
<ul>
<li>在这篇文章中，我们介绍了Johnson–Lindenstrauss引理（JL引理），它是关于降维的一个重要而奇妙的结论，是高维空间的不同寻常之处的重要体现之一。它告诉我们“只需要O(logN)维空间就可以塞下N个向量”，使得原本高维空间中的检索问题可以降低到O(logN)维空间中。</li>
</ul>
</li>
<li><p>Hierachical Softmax</p>
<ul>
<li>这是对full softmax的一种优化手段，Hierachical  Softmax的基本思想就是首先将词典中的每个词按照词频大小构建出一棵Huffman树，保证词频较大的词处于相对比较浅的层，词频较低的词相应的处于Huffman树较深层的叶子节点，每一个词都处于这棵Huffman树上的某个叶子节点，这样将原本的一个|V|分类问题变成了$\log|V|$  次的二分类问题。做法简单来说就是，原先要计算$P(w_t|c_t)$  的时候，因为使用的是普通的softmax，势必要求词典中的每一个词的概率大小，为了减少这一步的计算量，在Hierachical Softmax中，同样是计算当前词$w_t$在其上下文中的概率大小，只需要把它变成在Huffman树中的路径预测问题就可以了，因为当前词$w_t$在Huffman树中对应到一条路径，这条路径由这棵二叉树中从根节点开始，经过一系列中间的父节点，最终到达当前这个词的叶子节点而组成，那么在每一个父节点上，都对应的是一个二分类问题（本质上就是一个LR分类器），而Huffman树的构造过程保证了树的深度为$\log|V|$，所以也就只需要做$\log|V|$次二分类便可以求得的$P(w_t|c_t)$大小，这相比原来|V|次的计算量，已经大大减小了。</li>
</ul>
</li>
<li><p><a href="https://spaces.ac.cn/archives/3290" target="_blank" rel="noopener">寻求一个光滑的最大值函数</a> by 苏剑林 ✅</p>
<ul>
<li><script type="math/tex; mode=display">
\max (x, y, z, \ldots)=\lim _{k \rightarrow+\infty} \frac{1}{k} \ln \left(e^{k x}+e^{k y}+e^{k z}+\ldots\right)</script></li>
</ul>
</li>
<li><p>#TODO On Variational Bounds of Mutual Information <a href="https://arxiv.org/abs/1905.06922" target="_blank" rel="noopener">arXiv</a> (Citations <strong>164</strong>)</p>
</li>
</ul>
<h2 id="自监督和无监督学习"><a href="#自监督和无监督学习" class="headerlink" title="自监督和无监督学习"></a>自监督和无监督学习</h2><hr>
<h3 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h3><ul>
<li><p>#TODO On mutual information maximization for representation learning, ICLR, 2020, <a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">arXiv</a></p>
</li>
<li><p>#TODO Mine: mutual information neural estimation, ICML, 2018</p>
</li>
<li><p>#TODO Self-supervised Learning: Generative or Contrastive <a href="https://arxiv.org/abs/2006.08218" target="_blank" rel="noopener">arXiv</a> (Citations <strong>65</strong>)</p>
</li>
<li><p>#TODO <strong>Debiased Contrastive Learning</strong>, <strong>NIPS, 2020</strong></p>
</li>
<li><p>#TODO Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data</p>
</li>
<li><p>#TODO Learning representations by maximizing mutual information across views</p>
</li>
<li><p>#TODO A theoretical analysis of contrastive unsupervised representation learning, ICML</p>
</li>
<li><p><strong>Representation Learning with Contrastive Predictive Coding</strong> <a href="https://arxiv.org/abs/1807.03748" target="_blank" rel="noopener">arXiv</a> (Citations <strong>1078</strong>)</p>
<ul>
<li>一定窗口内的$x_t$和$x_{t+k}$为positive pair，随机采样一个$x_{t*}$作负例，为了把历史的信息也加入进去，作者提出可以在编码器上面再叠一个自回归模型，比如rnn这种，把编码器的输出$z_t$当作输入，这样可以在表示$c_t$中融入时序信息，拿$c_t$来做对比学习，下游任务既可以用$c_t$也可以用$z_t$，又或者是二者的融合。</li>
<li><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/948A9206-34B0-4EAB-8DDE-3DC9B5C1B9DE.png" alt="img" style="zoom: 25%;" /></li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/108625273" target="_blank" rel="noopener">Self-Supervised Learning 入门介绍</a> ✅</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/141141365" target="_blank" rel="noopener">对比学习（Contrastive Learning）相关进展梳理</a> ✅</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/72516633" target="_blank" rel="noopener">度量学习中的pair-based loss</a> ✅</p>
<ul>
<li>介绍了Contrastive loss、Triplet loss、Triplet center loss、N-pair loss(应该就是infoNCE)、Quadruplet loss、Lifted Structure loss</li>
<li>N-pair loss中相似度D如果是向量点积，就等价于InfoNCE</li>
<li><script type="math/tex; mode=display">
\mathcal{L}=\sum_{y_{i i}=1} \log \left(1+\sum_{y_{i k}=0} \exp \left(D_{i k}-D_{i i}\right)\right)</script></li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/149517352" target="_blank" rel="noopener">捋一捋 NCE</a> 公式推导比较详细，softmax中的分母计算量太大，NCE是是用一个二分类任务去逼近softmax的训练效果，推导证明了当k足够大时，采样k个负样本，NCE的目标函数和使用了softmax函数的最大似然是等价的。✅</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/6qqFAQBaOFuXtaeRSmQgsQ" target="_blank" rel="noopener">一文梳理2020年大热的对比学习模型</a> by 李rumor ✅</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/129076690" target="_blank" rel="noopener">理解Contrastive Predictive Coding和NCE Loss</a> ✅</p>
</li>
<li><p><a href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html" target="_blank" rel="noopener">Contrastive Self-Supervised Learning</a> ✅</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/370782081" target="_blank" rel="noopener">利用Contrastive Learning对抗数据噪声：对比学习在微博场景的实践</a> by 张俊林✅</p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/334772391" target="_blank" rel="noopener">Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE</a> ✅</p>
<ul>
<li><p>InfoNCE中这个式子的意思是说给定条件c，在一堆例子X中发现$x_{pos}$是正例概率，那就应该是$x_{pos}$被当成正例的概率除以所有其他sample被当成正例的概率</p>
</li>
<li><script type="math/tex; mode=display">
p(\operatorname{detect\ x_{pos}\ correctly} \mid X, \mathbf{c})=\frac{p\left(x_{\mathrm{pos}} \mid \mathbf{c}\right) \prod_{i=1, \ldots, N ; i \neq \mathrm{pos}} p\left(\mathbf{x}_{i}\right)}{\sum_{j=1}^{N}\left[p\left(\mathbf{x}_{j} \mid \mathbf{c}\right) \prod_{i=1, \ldots, N ; i \neq j} p\left(\mathbf{x}_{i}\right)\right]}=\frac{\frac{p\left(\mathbf{x}_{\mathrm{pos}} \mid c\right)}{p\left(x_{\mathrm{pos}}\right)}}{\sum_{j=1}^{N} \frac{p\left(\mathbf{x}_{j} \mid \mathbf{c}\right)}{p\left(\mathbf{x}_{\mathbf{j}}\right)}}=\frac{f\left(\mathbf{x}_{\mathrm{pos}}, \mathbf{c}\right)}{\sum_{j=1}^{N} f\left(\mathbf{x}_{j}, \mathbf{c}\right)}</script></li>
</ul>
</li>
<li><p><a href="https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html" target="_blank" rel="noopener">Contrastive Representation Learning</a> by lilian wen ✅</p>
<ul>
<li><p>Hard Negative Mining，因为我们并不知道负样本的真实分布概率，因此采样时有可能采样到正样本作为了负样本，即false negative，导致bias，下面的概率公式是对这种bias做了debias <a href="https://arxiv.org/abs/2007.00224" target="_blank" rel="noopener">Chuang et al., 2020</a></p>
</li>
<li><script type="math/tex; mode=display">
p\left(\mathbf{x}^{\prime}\right)=\eta^{+} p_{x}^{+}\left(\mathbf{x}^{\prime}\right)+\eta^{-} p_{x}^{-}\left(\mathbf{x}^{\prime}\right) \\
g\left(\mathbf{x},\left\{\mathbf{u}_{i}\right\}_{i=1}^{N},\left\{\mathbf{v}_{i}\right\}_{i=1}^{M}\right)=\max \left\{\frac{1}{\eta^{-}}\left(\frac{1}{N} \sum_{i=1}^{N} \exp \left(f(\mathbf{x})^{\top} f\left(\mathbf{u}_{i}\right)\right)-\frac{\eta^{+}}{M} \sum_{i=1}^{M} \exp \left(f(\mathbf{x})^{\top} f\left(\mathbf{v}_{i}\right)\right)\right), \exp (-1 / \tau)\right\}</script></li>
<li><p>下图源自<a href="https://arxiv.org/abs/2010.04592" target="_blank" rel="noopener">Robinson et al. (2021)</a></p>
</li>
<li><p><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/738686F5-7511-4722-8485-CA228F13C2FA.png" alt="738686F5-7511-4722-8485-CA228F13C2FA" style="zoom: 15%;" /></p>
</li>
</ul>
</li>
<li><p><strong>Understanding contrastive representation learning through alighment and uniformity on the hypersphere</strong>, <strong>Tongzhou Wang, ICML, 2020</strong> <a href="https://arxiv.org/abs/2005.10242" target="_blank" rel="noopener">arXiv</a> (Citations <strong>113</strong>) ✅</p>
</li>
<li><p><strong>Understanding the behaviour of contrastive loss</strong>, <strong>Feng Wang, CVPR, 2021</strong> <a href="https://arxiv.org/abs/2012.09740" target="_blank" rel="noopener">arXiv</a> (Citations <strong>6</strong>) ✅</p>
<ul>
<li><p>首先有</p>
</li>
<li><script type="math/tex; mode=display">
\begin{eqnarray}
\mathcal{L}\left(x_{i}\right) & = & -\log \left[\frac{\exp \left(s_{i, i} / \tau\right)}{\sum_{k \neq i} \exp \left(s_{i, k} / \tau\right)+\exp \left(s_{i, i} / \tau\right)}\right] \\
P_{i, j} & = & \frac{\exp \left(s_{i, j} / \tau\right)}{\sum_{k \neq i} \exp \left(s_{i, k} / \tau\right)+\exp \left(s_{i, i} / \tau\right)} \\

\end{eqnarray}</script></li>
<li><p>求梯度后可以得到</p>
</li>
<li><script type="math/tex; mode=display">
\begin{eqnarray}
\frac{\partial \mathcal{L}\left(x_{i}\right)}{\partial s_{i, i}}& = & \frac{\partial}{\partial s_{i, i}}\left[-\frac{1}{\tau}s_{i,i}+\log \left[\sum_{k \neq i}\exp (s_{i, k}/\tau) + \exp(s_{i,i}/\tau)\right]\right] \\
& = & -\frac{1}{\tau} + \frac{1}{\tau}\frac{\exp(s_{i,i}/\tau)}{\left[\sum_{k \neq i}\exp (s_{i, k}/\tau) + \exp(s_{i,i}/\tau)\right]} \\
& = & -\frac{1}{\tau} \sum_{k \neq i} P_{i, k} \\ 
& = & \frac{1}{\tau}(P_{i,i}-1) \\
\frac{\partial \mathcal{L}\left(x_{i}\right)}{\partial s_{i, j}}& = & \frac{1}{\tau} P_{i, j}
\end{eqnarray}</script></li>
<li><p>可以发现，如果$P_{i,i}=1$的时候，alignment就做到perfect了，而对于负例的梯度是与$P_{i,j}$正相关的，负例的相似度得分越大，梯度越大，也即对其的惩罚越大。对正例的梯度绝对值等于对所有负例的梯度之和。总结在2021.07.18的周报里，<a href="https://zhuanlan.zhihu.com/p/357071960" target="_blank" rel="noopener">知乎解读博客</a>，</p>
</li>
<li><p>如果太注重困难负样本则会破坏网络经过一定训练后已经学到的语义信息，这种情况在训练后期尤其明显。随着训练的进行，网络获取到的信息越来越接近真实语义特性，那么此时的负样本更有可能是潜在的正样本(false negative)，因此一个启示是可以随着迭代的次数增多而增大温度系数</p>
</li>
</ul>
</li>
<li><p><strong>A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING</strong> <a href="https://arxiv.org/abs/2011.00362" target="_blank" rel="noopener">arXiv</a> (Citations <strong>31</strong>) ✅ 感觉偏重对CV领域进展的介绍，对NLP的介绍不足</p>
</li>
</ul>
<h2 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h2><ul>
<li>#TODO <strong>Graph Neural Networks for Natural Language Processing: A Survey</strong> <a href="https://arxiv.org/abs/2106.06090" target="_blank" rel="noopener">arXiv</a></li>
</ul>
<h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><hr>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/321449610" target="_blank" rel="noopener">PyTorch 源码解读之 torch.autograd: 梯度计算详解</a> ✅</li>
<li><a href="https://blog.csdn.net/qq_43328040/article/details/108421469" target="_blank" rel="noopener">Autograd看这一篇就够了</a> ✅</li>
<li><a href="https://www.zhuanzhi.ai/document/9b1d28ae31d37c2fdf233140b47fbb54" target="_blank" rel="noopener">半小时学会 PyTorch Hook</a></li>
</ul>
<h2 id="AllenNLP"><a href="#AllenNLP" class="headerlink" title="AllenNLP"></a>AllenNLP</h2><hr>
<ul>
<li><a href="https://medium.com/ai2-blog/tutorial-training-on-larger-batches-with-less-memory-in-allennlp-1cd2047d92ad" target="_blank" rel="noopener">Tutorial: Training on larger batches with less memory in AllenNLP</a></li>
</ul>
<h2 id="Hugging-Face-Transformers"><a href="#Hugging-Face-Transformers" class="headerlink" title="Hugging Face Transformers"></a>Hugging Face Transformers</h2><hr>
<ul>
<li><a href="https://fancyerii.github.io/2021/05/11/huggingface-transformers-1/" target="_blank" rel="noopener">Huggingface Transformer教程(一)</a> ✅</li>
</ul>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><hr>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><hr>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">Adam那么棒，为什么还对SGD念念不忘 (1) —— 一个框架看懂优化算法</a> , 用一个框架总结了SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; RMSProp -&gt; Adam -&gt; Nadam，下面公式中：theta为参数，g为梯度，m为一阶动量，V为二阶动量，alpha为学习率，beta_1为一阶动量系数，beta_2为二阶动量系数，<strong>下面的公式与作者原文略有出入</strong> ✅</p>
<ul>
<li><p>SGD with momentum，考虑了大约$1/(1-\beta_1)$个时刻的梯度的平均值: </p>
<script type="math/tex; mode=display">
\begin{aligned}
m_{t+1} &=\beta_{1} \cdot m_{t} - \alpha \cdot \nabla_{\theta_{t}} L\left(\theta_{t}\right) \\
\theta_{t+1} &= \theta_{t} + m_{t+1} \\
&= \theta_{t} + \beta_{1} \cdot m_{t} - \alpha \cdot \nabla_{\theta_{t}} L\left(\theta_{t}\right)
\end{aligned}</script></li>
<li><p>SGD with Nesterov Acceleration，不直接计算当前位置的梯度方向，而是先计算按照累积动量走了一步那个时候的梯度方向，然后用这个点的梯度方向，与历史累积动量相结合做梯度下降。<strong>为什么要这么做呢？很明显在momentum里，我们知道当前参数一定要根据上一时刻的动量走一步，也就是 $ \beta_{1} \cdot m_{t} $ ，既然我都知道 $\theta$ 一定会走这步，为什么我不先走了之后再根据那的梯度再走呢？</strong>这主要是为了解决momentum梯度下降冲过头的情况，关于PyTorch实现Nesterov方式的<a href="https://github.com/lisa-lab/pylearn2/pull/136#issuecomment-10381617" target="_blank" rel="noopener">讨论1:提出了nesterov的等价形式</a>与<a href="https://github.com/pytorch/pytorch/pull/887#issuecomment-569087543" target="_blank" rel="noopener">讨论2:总结了一些issue</a>与<a href="https://raw.githubusercontent.com/fidlej/optim/master/dok/nesterov_simple.pdf" target="_blank" rel="noopener">简单推导pdf</a>，下面是详细<a href="https://blog.csdn.net/u012328159/article/details/80311892" target="_blank" rel="noopener">推导</a>，首先是Nesterov原公式</p>
<script type="math/tex; mode=display">
\begin{aligned}
m_{t+1} &=\beta \cdot  m_{t}-\alpha \cdot \nabla_{\theta_{t}} L\left(\theta_{t}+\beta \cdot m_{t}\right) \\
\theta_{t+1} &=\theta_{t}+m_{t+1}
\end{aligned}</script><p>下面是其“等价”形式（至于为什么等价我还未完全明白），也是各大框架实现它的方式：首先令$\theta_{t}^{\prime}=\theta_{t}+\beta \cdot m_{t}$，则$m_{t+1}=\beta \cdot m_{t}-\alpha \nabla_{\theta_{t}} L\left(\theta_{t}^{\prime}\right)$，则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1}^{\prime} &=\theta_{t+1}+\beta \cdot m_{t+1} \\
&=\theta_{t}+m_{t+1}+\beta \cdot m_{t+1} \\
&=\theta_{t}^{\prime}-\beta \cdot m_{t} +\beta \cdot m_{t}-\alpha \nabla_{\theta_{t}} L\left(\theta_{t}^{\prime}\right) + \beta \cdot m_{t+1} \\
&= \theta_{t}^{\prime} + \beta \cdot m_{t+1} - \alpha \nabla_{\theta_{t}} L\left(\theta_{t}^{\prime}\right) ——\operatorname{PyTorch/Keras~implementation} \\
&=\theta_{t}^{\prime}+\beta^{2} \cdot m_{t}-(1+\beta) \alpha \nabla_{\theta_{t}} L\left(\theta_{t}^{\prime}\right) ——\operatorname{Another~way~of~implementation}
\end{aligned}</script><p>最后再令$\theta_t=\theta^{\prime}_{t}$，就得到了$\theta_{t+1}=\theta_{t}+\beta^{2} \cdot m_{t}-(1+\beta) \cdot \alpha \nabla_{\theta_{t}} L\left(\theta_{t}\right)$，下面贴一张keras的实现代码</p>
<p><img src="https://gitee.com/TianHongZXY/blog-images/raw/master/20180519100022431.jpeg" alt="keras-nesterov"></p>
<p><strong>另一种视角，NAG本质上是多考虑了目标函数的二阶导信息：<a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a> </strong>by 郑华滨 ✅</p>
<script type="math/tex; mode=display">
\begin{aligned}
m_{t+1} &=\beta \cdot  m_{t} + \nabla_{\theta_{t}} L\left(\theta_{t}\right) + \beta \left[ \nabla_{\theta_{t}} L\left(\theta_{t}\right) - \nabla_{\theta_{t-1}} L\left(\theta_{t-1}\right) \right] \\
\theta_{t+1} &=\theta_{t} - \alpha \cdot m_{t+1}
\end{aligned}</script><p><a href="https://blogs.princeton.edu/imabandit/2015/06/30/revisiting-nesterovs-acceleration" target="_blank" rel="noopener">Princeton关于nesterovs的博客</a></p>
</li>
<li><p>AdaGrad，学习率除以累积二阶动量，被更新越多的参数学习率越小，存在的问题是到最后二阶动量一直累积，导致学习率接近0，可能导致训练提前结束:</p>
<script type="math/tex; mode=display">
V_{t}=\sum_{\tau=1}^{t} g_{\tau}^{2} \\
\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}</script></li>
<li><p>RMSProp，修正AdaGrad的问题，只考虑过去一段时间的二阶动量:</p>
<script type="math/tex; mode=display">
V_{t}=\beta_{2} * V_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}</script></li>
<li><p>Adam，结合SGD的一阶动量与RMSProp的二阶动量更新公式，一般$\beta_1=0.9,\beta_2=0.999,m_0=0,V_0=0$，初期m和V都太接近0，因而使用下式进行误差修正:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\tilde{m}_{t}=m_{t} /\left(1-\beta_{1}^{t}\right) \\
&\tilde{V}_{t}=V_{t} /\left(1-\beta_{2}^{t}\right)
\end{aligned}</script></li>
<li><p>NAdam，结合Adam与Nesterov Acceleration</p>
</li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/32338983" target="_blank" rel="noopener">Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略</a> ✅</p>
<ul>
<li>稀疏数据优先考虑自适应学习率算法；使用自适应学习率算法一定要shuffle数据；可以先用小数据集实验；当验证集指标不变或下降时降低学习率</li>
</ul>
</li>
<li><p><a href="https://blog.csdn.net/u012328159/article/details/80311892" target="_blank" rel="noopener">深度学习中优化方法——momentum、Nesterov Momentum、AdaGrad、Adadelta、RMSprop、Adam</a> ，公式推导详尽，也给了示例代码和很多深度学习框架的源码，不错的一篇文章</p>
</li>
<li><p><a href="https://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a> by Sebastian Ruder</p>
</li>
</ul>
<h3 id="Fine-tune-trick"><a href="#Fine-tune-trick" class="headerlink" title="Fine tune trick"></a>Fine tune trick</h3><hr>
<ul>
<li><p>Discriminative fine-tuning</p>
<ul>
<li><p>基本思想是针对不同的层在训练更新参数的时候，赋予不同的学习率。这里的出发点是，一般来说，对于NLP的深度学习模型来说，不同层的表征有不同的物理含义，比如浅层偏句法信息，高层偏语义信息，因此对于不同层的学习率不同，自然就是比较合理的了。先指定最后一层的学习率，然后根据下式得到前面层的学习率，基本思想是让浅层的学习率要更小一些。</p>
</li>
<li><script type="math/tex; mode=display">
\begin{aligned}
\theta_{t}^{l} &= \theta_{t-1}^{l}+\eta^{l} \nabla_{\theta^{l}} J(\theta) \\
\eta^{l-1} &= \frac{\eta^l}{2.6}
\end{aligned}</script></li>
</ul>
</li>
<li><p>Slanted triangular learning rates</p>
<ul>
<li>类似于warm up，学习率先从0开始线性增加，然后线性减小，因此学习率画出图来是一个斜三角形</li>
</ul>
</li>
<li><p>Gradual unfreezing</p>
<ul>
<li>主要思想是把预训练的模型在新任务上finetune时，逐层解冻模型，也就是先finetune最后一层，然后再解冻倒数第二层，把倒数第二层和最后一层一起finetune，然后再解冻第三层，以此类推，逐层往浅层推进，最终finetune整个模型或者终止到某个中间层。这样做的目的也是为了finetune的过程能够更平稳。</li>
</ul>
</li>
</ul>
<h3 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h3><hr>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/33173246" target="_blank" rel="noopener">详解深度学习中的Normalization，BN/LN/WN</a> ✅</li>
</ul>
<h2 id="他人撰写或收集的资料"><a href="#他人撰写或收集的资料" class="headerlink" title="他人撰写或收集的资料"></a>他人撰写或收集的资料</h2><hr>
<ul>
<li><a href="https://github.com/DA-southampton/NLP_ability" target="_blank" rel="noopener">NLP-ability</a>: 总结梳理自然语言处理工程师(NLP)需要积累的各方面知识，包括面试题，各种基础知识，工程能力等等，提升核心竞争力 by DASOU</li>
<li><a href="https://fancyerii.github.io/" target="_blank" rel="noopener">李理的博客</a></li>
<li><a href="https://github.com/zhijing-jin/nlp-phd-global-equality#" target="_blank" rel="noopener">Resources to Help Global Equality for PhDs in NLP / AI</a></li>
<li><a href="https://conanhujinming.github.io/comments-for-awesome-courses/" target="_blank" rel="noopener">名校公开课程评价网</a>, <a href="https://github.com/conanhujinming/comments-for-awesome-courses/" target="_blank" rel="noopener">项目github地址</a></li>
<li><a href="https://missing-semester-cn.github.io/" target="_blank" rel="noopener">MIT 计算机教育中缺失的一课 中文版</a>，<a href="https://missing.csail.mit.edu/" target="_blank" rel="noopener">英文原版</a>，<a href="https://www.youtube.com/playlist?list=PLyzOVJj3bHQuloKGG59rS43e29ro7I57J" target="_blank" rel="noopener">Video</a></li>
<li><a href="https://bicmr.pku.edu.cn/~wenzw/optbook.html" target="_blank" rel="noopener">最优化：建模、算法与理论</a></li>
</ul>
<h3 id="知乎专栏"><a href="#知乎专栏" class="headerlink" title="知乎专栏"></a>知乎专栏</h3><hr>
<ul>
<li><a href="https://www.zhihu.com/column/bzWeAreYoung" target="_blank" rel="noopener">醒醒啊，工头喊你搬砖了</a> by <a href="https://www.zhihu.com/people/jiang-a-sheng-85" target="_blank" rel="noopener">TheLongGoodbye</a>，内容主要关于NLP，作者为哈工大博士</li>
<li><a href="https://zhuanlan.zhihu.com/p/22464594" target="_blank" rel="noopener">无痛的机器学习</a> by <a href="https://www.zhihu.com/people/feng-chao" target="_blank" rel="noopener">冯超</a></li>
</ul>
<h3 id="PaperList"><a href="#PaperList" class="headerlink" title="PaperList"></a>PaperList</h3><hr>
<ul>
<li><a href="https://github.com/ContrastiveSR/Contrastive_Learning_Papers" target="_blank" rel="noopener">Contrastive Learning Papers</a></li>
<li><a href="https://github.com/TobiasLee/Awesome-Efficient-PLM" target="_blank" rel="noopener">Awesome-Efficient-PLM</a></li>
<li><a href="https://github.com/fulifeng/Causal_Reading_Group#causality-in-nlp" target="_blank" rel="noopener">Causal Reading Group</a></li>
</ul>
<h3 id="Github-Repo"><a href="#Github-Repo" class="headerlink" title="Github Repo"></a>Github Repo</h3><hr>
<ul>
<li><a href="https://github.com/Dod-o/Statistical-Learning-Method_Code" target="_blank" rel="noopener">手写实现李航《统计学习方法》书中全部算法</a></li>
<li><a href="https://github.com/0voice/interview_internal_reference" target="_blank" rel="noopener">2021年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总</a></li>
<li><a href="https://github.com/EssayKillerBrain/EssayKiller_V2" target="_blank" rel="noopener">基于开源GPT2.0的初代创作型人工智能 | 可扩展、可进化</a></li>
<li><a href="https://github.com/wzy6642/Machine-Learning-in-Action-Python3" target="_blank" rel="noopener">《机器学习实战》的python3源码</a></li>
</ul>
<h2 id="其他领域有趣的paper和工具"><a href="#其他领域有趣的paper和工具" class="headerlink" title="其他领域有趣的paper和工具"></a>其他领域有趣的paper和工具</h2><hr>
<ul>
<li><a href="https://sojo.im/slscq/" target="_blank" rel="noopener">申论生成器</a> <a href="[lab.magiconch.com/slscq](https://lab.magiconch.com/slscq/">网址2</a>) <a href="https://github.com/Uahh/slscq" target="_blank" rel="noopener">代码地址</a></li>
</ul>
<p>PaperRobot: Incremental Draft Generation of Scientific Ideas <a href="https://arxiv.org/abs/1905.07870" target="_blank" rel="noopener">arXiv</a></p>
<h3 id="苏剑林推荐论文"><a href="#苏剑林推荐论文" class="headerlink" title="苏剑林推荐论文"></a>苏剑林推荐论文</h3><ul>
<li>Fast Model Editing at Scale</li>
<li>Momentum Contrastive Autoencoder- Using Contrastive Learning for Latent Space Distribution Matching in WAE</li>
<li>On some theoretical limitations of Generative Adversarial Networks</li>
<li>SOFT- Softmax-free Transformer with Linear Complexit</li>
<li>Conditional Poisson Stochastic Beam Search</li>
<li>Sample Efficient Model Evaluation</li>
<li>Scale Efficiently- Insights from Pre-training and Fine-tuning Transformers</li>
<li>Controlled Text Generation as Continuous Optimization with Multiple Constraints</li>
<li>Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization</li>
<li>FMMformer- Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention</li>
<li>The Separation Capacity of Random Neural Network</li>
<li>Towards Zero-shot Language Modeling</li>
<li>Noisy Channel Language Model Prompting for Few-Shot Text Classification</li>
<li>Long-Short Transformer- Efficient Transformers for Language and Vision</li>
<li>Rethinking positional encoding</li>
<li>Simpler, Faster, Stronger- Breaking The log-K Curse On Contrastive Learners With FlatNCE</li>
<li>Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization</li>
<li>Variational Diffusion Models</li>
<li>Alias-Free Generative Adversarial Networks</li>
<li>Charformer- Fast Character Transformers via Gradient-based Subword Tokenization</li>
<li>Conjugate Energy-Based Models</li>
<li>Re-parameterizing VAEs for stability</li>
<li>Stable, Fast and Accurate- Kernelized Attention with Relative Positional Encoding</li>
</ul>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><h3 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h3><h4 id="ycm"><a href="#ycm" class="headerlink" title="ycm"></a>ycm</h4><ul>
<li>brew进行upgrade或者安装包时有可能break ycm及对应的python包，解决方案：进入youcompleteme目录，用新的python install.py，注意不可用anaconda版python <a href="https://blog.csdn.net/think_ycx/article/details/83659044" target="_blank" rel="noopener">CSDN</a></li>
<li>更换g++或gcc版本时有可能break ycm的环境，需要重新指定CMAKE_C_COMPILER路径，也需要用python重装ycm</li>
<li><a href="https://github.com/Homebrew/brew/issues/2356" target="_blank" rel="noopener">https://github.com/Homebrew/brew/issues/2356</a></li>
</ul>
<h3 id="Unix实用命令"><a href="#Unix实用命令" class="headerlink" title="Unix实用命令"></a>Unix实用命令</h3><ul>
<li><a href="https://github.com/RenShuhuai-Andy/gpu_lurker" target="_blank" rel="noopener">服务器 GPU 监控程序，当 GPU 属性满足预设条件时通过微信发送提示消息</a></li>
</ul>
<h4 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h4><ul>
<li>mac端使用rsync指定远程端口传输文件的命令为：rsync -e ‘ssh -p PORT’</li>
</ul>
<h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><ul>
<li>端口转发，在远程服务器连接本机的梯子，本机的梯子代理端口为localhost:7890，则在本机连接远程服务器 <code>ssh -R localhost:7890:localhost:7890 username@remote-server-ip</code>，然后在远程服务器运行<code>export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7890</code>，测试是否已经挂上本机的梯子<code>wget www.google.com</code></li>
</ul>
<h4 id="paste"><a href="#paste" class="headerlink" title="paste"></a>paste</h4><ul>
<li>可以将文件所有行按分隔符合并成一行，或者按分隔符合并两个文件，例子：Merge two files side by side, each in its column, using the specified delimiter，<code>paste -d   </code>，基本用法见tldr</li>
</ul>
<h4 id="kill"><a href="#kill" class="headerlink" title="kill"></a>kill</h4><ul>
<li>根据进程名称关键词，批量打印所有要kill的进程，<code>ps -ef | grep  | grep -v grep | awk &#39;{print &quot;kill -9 &quot;$2}&#39;</code>，批量kill这些进程 <code>ps -ef | grep  | grep -v grep | awk &#39;{print &quot;kill -9 &quot;$2}&#39; | sh</code></li>
</ul>
<h2 id="科研技巧"><a href="#科研技巧" class="headerlink" title="科研技巧"></a>科研技巧</h2><h3 id="三遍读论文-by-李沐"><a href="#三遍读论文-by-李沐" class="headerlink" title="三遍读论文(by 李沐)"></a>三遍读论文(by 李沐)</h3><ul>
<li>第一遍读title，abstract和conclusion，看看是否是自己感兴趣的以及效果怎么样，可以再看一看method和experiment里的图表</li>
<li>确定是自己感兴趣的之后，第二遍从头开始读，不要太注意细节比如公式和证明，但要完全理解重要的图和表都在干什么，作者是如何与别人对比的，差距有多大，可以把作者提到的重要相关文献圈出来，比如作者做的问题就是这些文献里提出来的，或者作者是根据这些文献进行的改进</li>
<li>第三遍，这篇文章值得完全搞懂，要明白文章的每一句话，并且想象自己在实现这篇文章，作者根据xx问题提出的xx方法，如果换我来解决这个问题我怎么做，作者说留着future做的工作，如果我来做怎么做</li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a>
          
            <a href="/tags/machine-learning-NLP/" rel="tag"><i class="fa fa-tag"></i> machine learning - NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/01/31/%E3%80%90EMNLP2020%E3%80%91%E6%8E%A7%E5%88%B6%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90%E4%B8%AD%E7%9A%84specificity/" rel="next" title="【EMNLP2020】控制对话生成中的specificity">
                <i class="fa fa-chevron-left"></i> 【EMNLP2020】控制对话生成中的specificity
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/06/20/NCE/" rel="prev" title="《NCE与InfoNCE》">
                《NCE与InfoNCE》 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/header.jpg"
                alt="TianHongZXY"/>
            
              <p class="site-author-name" itemprop="name">TianHongZXY</p>
              <p class="site-description motion-element" itemprop="description">浪漫骑士 行吟诗人 自由思想者</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/%20%7C%7C%20archive">
                
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/TianHongZXY" title="GitHub &rarr; https://github.com/TianHongZXY" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/mailto:tianhongzxy@163.com" title="E-Mail &rarr; mailto:tianhongzxy@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.zhihu.com/people/tianhongzxy" title="知乎 &rarr; https://www.zhihu.com/people/tianhongzxy" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>知乎</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#小样本学习"><span class="nav-number">1.</span> <span class="nav-text">小样本学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NLP"><span class="nav-number">2.</span> <span class="nav-text">NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#任务型对话"><span class="nav-number">2.1.</span> <span class="nav-text">任务型对话</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#小样本"><span class="nav-number">2.1.1.</span> <span class="nav-text">小样本</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#开放域对话"><span class="nav-number">2.2.</span> <span class="nav-text">开放域对话</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问答系统"><span class="nav-number">2.3.</span> <span class="nav-text">问答系统</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformers"><span class="nav-number">2.4.</span> <span class="nav-text">Transformers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文本生成"><span class="nav-number">2.5.</span> <span class="nav-text">文本生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#词表征"><span class="nav-number">2.6.</span> <span class="nav-text">词表征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#有监督"><span class="nav-number">2.6.1.</span> <span class="nav-text">有监督</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无监督"><span class="nav-number">2.6.2.</span> <span class="nav-text">无监督</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#句子表征"><span class="nav-number">2.7.</span> <span class="nav-text">句子表征</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#有监督-1"><span class="nav-number">2.7.1.</span> <span class="nav-text">有监督</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#无监督-1"><span class="nav-number">2.7.2.</span> <span class="nav-text">无监督</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CV"><span class="nav-number">3.</span> <span class="nav-text">CV</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图片表征"><span class="nav-number">3.1.</span> <span class="nav-text">图片表征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重参数化"><span class="nav-number">4.</span> <span class="nav-text">重参数化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#知识蒸馏"><span class="nav-number">5.</span> <span class="nav-text">知识蒸馏</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE"><span class="nav-number">6.</span> <span class="nav-text">VAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对抗训练"><span class="nav-number">7.</span> <span class="nav-text">对抗训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数学相关"><span class="nav-number">8.</span> <span class="nav-text">数学相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自监督和无监督学习"><span class="nav-number">9.</span> <span class="nav-text">自监督和无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对比学习"><span class="nav-number">9.1.</span> <span class="nav-text">对比学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图神经网络"><span class="nav-number">10.</span> <span class="nav-text">图神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PyTorch"><span class="nav-number">11.</span> <span class="nav-text">PyTorch</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AllenNLP"><span class="nav-number">12.</span> <span class="nav-text">AllenNLP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hugging-Face-Transformers"><span class="nav-number">13.</span> <span class="nav-text">Hugging Face Transformers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">14.</span> <span class="nav-text">机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#优化算法"><span class="nav-number">14.1.</span> <span class="nav-text">优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fine-tune-trick"><span class="nav-number">14.2.</span> <span class="nav-text">Fine tune trick</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#杂项"><span class="nav-number">14.3.</span> <span class="nav-text">杂项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#他人撰写或收集的资料"><span class="nav-number">15.</span> <span class="nav-text">他人撰写或收集的资料</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#知乎专栏"><span class="nav-number">15.1.</span> <span class="nav-text">知乎专栏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PaperList"><span class="nav-number">15.2.</span> <span class="nav-text">PaperList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Github-Repo"><span class="nav-number">15.3.</span> <span class="nav-text">Github Repo</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他领域有趣的paper和工具"><span class="nav-number">16.</span> <span class="nav-text">其他领域有趣的paper和工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#苏剑林推荐论文"><span class="nav-number">16.1.</span> <span class="nav-text">苏剑林推荐论文</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#工具"><span class="nav-number">17.</span> <span class="nav-text">工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vim"><span class="nav-number">17.1.</span> <span class="nav-text">Vim</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ycm"><span class="nav-number">17.1.1.</span> <span class="nav-text">ycm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unix实用命令"><span class="nav-number">17.2.</span> <span class="nav-text">Unix实用命令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scp"><span class="nav-number">17.2.1.</span> <span class="nav-text">scp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ssh"><span class="nav-number">17.2.2.</span> <span class="nav-text">ssh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#paste"><span class="nav-number">17.2.3.</span> <span class="nav-text">paste</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kill"><span class="nav-number">17.2.4.</span> <span class="nav-text">kill</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#科研技巧"><span class="nav-number">18.</span> <span class="nav-text">科研技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#三遍读论文-by-李沐"><span class="nav-number">18.1.</span> <span class="nav-text">三遍读论文(by 李沐)</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 – <span itemprop="copyrightYear">2021</span>
  <!-- <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span> -->
  <span class="author" itemprop="copyrightHolder"><span class="with-love"><i class="fa fa-heart-o"></i></span>TianHongZXY</span>

  

  
</div>
<!--

  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v6.7.0</div>

-->


        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  

  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  








  
  
  
  
  <script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  <script>
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item) > -1;
    });
    new Valine({
      el: '#comments' ,
      verify: false,
      notify: false,
      appId: 'fNjvG519rNMQCRTez4XP1aGe-gzGzoHsz',
      appKey: 'DY0fa7oCKanbyW5J4UoxG9ug',
      placeholder: 'Say something',
      avatar: 'mm',
      meta:guest,
      pageSize: '10' || 10,
      visitor: false
    });
  </script>




  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>
  <script>
    
    
  </script>


  

  

  

  

  

  

  

  
  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/click.js"></script>
</body>
</html>
